<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>test2</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="introduction">Test</h1>
<p>Schizophrenia is a chronic and serious mental disorder which affects how a person thinks, feels, and behaves. Although there have been many studies about psychological and behavioral manifestations of schizophrenia, neuroscientists have yet to determine a set of corresponding neurological biomarkers for this disorder. A functional magnetic resonance imaging (fMRI) can help determine non-invasive biomarkers for schizophrenia in brain function<a href="#ref1">[1]</a><a href="#ref2">[2]</a> and one such fMRI analysis technique called, dynamic functional network connectivity (dFNC)<a href="#ref3">[3]</a><a href="#ref4">[4]</a><a href="#ref10">[10]</a>, uses K-Means clustering to characterize time-varying connectivity between functional networks. Researchers have worked on finding correlation between schizophrenia and dFNC<a href="#ref1">[1]</a><a href="#ref2">[2]</a><a href="#ref5">[5]</a><a href="#ref6">[6]</a>, but little work has been done with the choice of clustering algorithm<a href="#ref7">[7]</a><a href="#ref9">[9]</a>. Therefore, in this project, we have studied how modifying the clustering technique in the dFNC pipeline can yield dynamic states from fMRI data that impact the accuracy of classifying schizophrenia<a href="#ref8">[8]</a>.</p>
<p>We experimented with DBSCAN, Hiearcharial Clustering, Gaussian Mixture Models, and Bayesian Gaussian Mixture Models clustering methods on subject connectivity matrices produced from fMRI data, and each algorithmâ€™s cluster assignments as features for SVMs, MLP, Nearest Neighbor, and other supervised classification algorithms to classify schizophrenia.</p>
<p>Section II describes the fMRI data used in our experimentation, while Section III summarizes the aforementioned clustering and classification algorithms used in the pipeline. Section IV compares the accuracy of these classifiers, along with presenting a series of charts that analyze the cluster assignments produced on the fMRI data.</p>
<h1 id="section-ii-data">Section II: Data</h1>
<p>All datasets used in this project are derivatives of fMRI data (functional magnetic resonance imaging), which is data measuring brain activity to track a patientâ€™s thought processs over a predefined time period.</p>
<h3 id="gaussian-simulated-dataset">Gaussian Simulated Dataset</h3>
<p>We derive our own synthetic dataset in order to test out our clusterers and classifiers on a simulated dataset for sanity checking their implementation.</p>
<p>The data set was generated by a seed set of <span class="math inline"><em>M</em>â€„=â€„50</span> source gaussian signals, which were organized into <span class="math inline"><em>k</em>â€„=â€„5</span> â€œconnectivity statesâ€. Each of the 5 connectivity states is a block-diagonal matrix with a block of size <span class="math inline"><em>M</em>/<em>k</em></span> which is located on the <span class="math inline"><em>i</em>â€…*â€…<em>M</em>/<em>k</em>â€…+â€…<em>M</em>/<em>k</em></span>th entry with a size of <span class="math inline"><em>i</em>â€…*â€…<em>M</em>/<em>k</em></span>. For each state, the set the pairs of relevant source signals to come from the same distribution, so that their correlation without additional additive noise is 1.</p>
<p>For each subject, we generate a source signal of sime <span class="math inline"><em>M</em>â€…Ã—â€…<em>T</em>â€…âˆ’â€…<em>W</em></span> where <span class="math inline"><em>T</em></span> is the length of the time-course, and <span class="math inline"><em>W</em></span> is the size of the sliding window. Over each window, we randomly select which state the window belongs to, drawing from a prior distribution of state probabilities for a subjectâ€™s class, <span class="math inline"><em>P</em><sub><em>c</em>,â€†<em>k</em></sub></span>. Additionally, for each class we simulate a transition probability <span class="math inline"><em>Q</em><sub><em>c</em>,â€†<em>k</em></sub></span> for each state, with higher probabilities meaning a higher chance of transitioning out of that state into some other state. Finally, we restrict the simulation such that transitions can only occur after a state has existed for a <span class="math inline"><em>W</em></span>-many timepoints. This restrictions allows us to assert with some certainty that each of the seed states will occur within some window for that subject.</p>
<p>For each class, we simulate baseline signal noise from a normal distribution, which the relevant source signal being added to that baseline.</p>
<p>Formally a single subjects signal within a window of size W is given as:</p>
<p><br /><span class="math display"><em>X</em><sub><em>i</em></sub>â€„=â€„ğ’©(<em>Î¸</em><sub><em>c</em></sub>)â€…+â€…ğ’©(<em>Î¸</em><sub><em>k</em></sub>)</span><br /></p>
<p>where <span class="math inline"><em>Î¸</em><sub><em>c</em></sub></span> and <span class="math inline"><em>Î¸</em><sub><em>k</em></sub></span> are the parameters for the baseline class signal for class <span class="math inline"><em>c</em></span> and the source signal for state <span class="math inline"><em>k</em></span> respectively. Because all source signals for state <span class="math inline"><em>k</em></span> will have the same distribution, their correlation will be high, providing the same block-matrix correlation for the dFNC analysis.</p>
<p>The probability for entering from state <span class="math inline"><em>k</em></span> into a new state <span class="math inline"><em>k</em>â€²</span> at a timepoint <span class="math inline"><em>T</em></span>, given that the last transition occured <span class="math inline"><em>W</em></span> time-points ago, is given as the joint probability of the prior distribution for the class ever entering into state <span class="math inline"><em>k</em>â€²</span>, as well as the probability of transitioning out of state <span class="math inline"><em>k</em></span>. i.e. <br /><span class="math display"><em>Z</em><sub><em>c</em>,â€†<em>k</em>,â€†<em>k</em>â€²</sub>(<em>P</em><sub><em>c</em>,â€†<em>k</em>â€²</sub>,â€†<em>Q</em><sub><em>c</em>,â€†<em>k</em></sub>)</span><br /></p>
<p>The addition of noise, and the fact that windows are created with a size <span class="math inline"><em>W</em></span> at each timepoint means that there is a high chance for bleed-over between the actual states detected for an individual subject.</p>
<p>For example the following connectivity matrices were computed from randomly selected subjects from each class, for each state:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Class</th>
<th style="text-align: center;">State 0</th>
<th style="text-align: center;">State 1</th>
<th style="text-align: center;">State 2</th>
<th style="text-align: center;">State 3</th>
<th style="text-align: center;">State 4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;"><img width="100%" src="data/examples/gauss/class0_state0.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="data/examples/gauss/class0_state1.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="data/examples/gauss/class0_state2.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="data/examples/gauss/class0_state3.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="data/examples/gauss/class0_state4.png?raw=True" /></td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;"><img width="100%" src="data/examples/gauss/class1_state0.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="data/examples/gauss/class1_state1.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="data/examples/gauss/class1_state2.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="data/examples/gauss/class1_state3.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="data/examples/gauss/class1_state4.png?raw=True" /></td>
</tr>
</tbody>
</table>
<p>For our simulation we generate <span class="math inline"><em>C</em>â€„=â€„2</span> classes, <span class="math inline"><em>K</em>â€„=â€„5</span> states, for <span class="math inline"><em>N</em>â€„=â€„300</span> subjects with <span class="math inline"><em>M</em>â€„=â€„50</span> source signals. The parameters <span class="math inline"><em>Q</em></span> were set at 0.5 for each class and each state, so that transitioning out of states was equally likely for all classes and states. Baseline noise was set at <span class="math inline"><em>Ïƒ</em><sub><em>c</em></sub>â€„=â€„1â€…Ã—â€…10<sup>â€…âˆ’â€…2</sup></span> for each class, and the <span class="math inline"><em>s</em><em>i</em><em>g</em><em>m</em><em>a</em><sub><em>k</em></sub></span> for all source signals was set at <span class="math inline"><em>s</em><em>i</em><em>g</em><em>m</em><em>a</em><sub><em>k</em></sub>â€„=â€„1â€…Ã—â€…10<sup>â€…âˆ’â€…1</sup></span>.</p>
<p>The parameters for <span class="math inline"><em>P</em></span> for each class were selected as:</p>
<table>
<thead>
<tr class="header">
<th>Class Parameter</th>
<th>State 0</th>
<th>State 1</th>
<th>State 2</th>
<th>State 3</th>
<th>State 4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline"><em>P<sub>(c=0,k)</sub></em></span></td>
<td>0.4</td>
<td>0.1</td>
<td>0.1</td>
<td>0.3</td>
<td>0.1</td>
</tr>
<tr class="even">
<td><span class="math inline"><em>P<sub>(c=1,k)</sub></em></span></td>
<td>0.1</td>
<td>0.1</td>
<td>0.3</td>
<td>0.1</td>
<td>0.4</td>
</tr>
</tbody>
</table>
<h3 id="fbirn-dataset">FBIRN Dataset</h3>
<p>We use derivatives from the Phase 3 Dataset of FBIRN (Functional Biomedical Infromatics Research Network Data Repository), which specifically focuses on brain activity maps from patients with schizophrenia. This dataset includes 186 healthy controls and 176 indivduals from schizophrenia from around the United States. Subject participants in this dataset are between the ages of 18-62.</p>
<h3 id="ucla-dataset">UCLA Dataset</h3>
<p>We also use derivatives from the UCLA Consortium for Neuropsychiatric Phenomics archives, which includes neuroimages for roughly 272 participants. The subject population consists of roughly 272 healthy controls, as well as participants with a diagnosis of schizophrenia (50 subjects). Subject participants in this dataset range from 21 to 50 years</p>
<h1 id="section-iii-methods">Section III: Methods</h1>
<h2 id="overview">Overview</h2>
<p><img width="100%" src="results/dfnc_pipeline(1).png?raw=True" /></p>
<h2 id="preprocessing">Preprocessing</h2>
<p>As is standard in Dynamic Functional Network Connectivity, Group Independent Component Analysis of subject images is used to compute a set of <span class="math inline"><em>M</em></span> statistically independent spatial maps from the data, along with a set of <span class="math inline"><em>M</em></span> time-courses, which are used for the dFNC analysis. For the Fbirn data, we used pre-computed ICA timecourses provided by Damaraju et al.Â 2014. These timecourses were computed using Infomax ICA [] with 100 components, which were then manually selected based on biological relevance for a total of 47 remaining components. For the UCLA data, because no ICA derivatives were readily available, we used Group Information Guided ICA [] which is included as part of the GIFT NeuroImaging Analysis Toolbox []. As a template for GIG-ICA, we used the NeuroMark template [], which has been shown to provide reliable estimations of components in many different settings. This provides us with 53 independent components for Uhe UCLA data.</p>
<h3 id="sliding-window-analysis">Sliding Window Analysis</h3>
<p>For all data-sets we chose a sliding window of size <span class="math inline"><em>W</em>â€„=â€„22</span>, following the precedent in Damaraju et al.Â 2014 []. Time-Series data <span class="math inline"><em>X</em></span> with size <span class="math inline"><em>M</em>â€…Ã—â€…<em>T</em></span>, where <span class="math inline"><em>M</em></span> is the number of independent components, and <span class="math inline"><em>T</em></span> is the number of timepoints are used. Time series were normalized and convolved with a gaussian window with a kernel of size 0.015, again following the precedent in Damaraju et al.Â 2014. For each window on each time-series a number of exemplar data-points were selected by taking the local maximum of the variance from the smoothed time-series.</p>
<p>We computed correlation coefficients across the components, within each time series window to form the FNC matrix with entries <span class="math inline"><em>i</em></span> and <span class="math inline"><em>j</em></span> given as</p>
<p><br /><span class="math display">$$
  FNC_{i,j} = \frac{1}{W}  \frac{\sum_{w=1}^{W} X_i X_j - (\sum_{w=1}^{W}X_i)(\sum_{w=1}^{W}X_j)  }{W \sqrt{(\sum X_i^2 - (\sum X_i)^2)(\sum X_j^2 - (\sum X_j)^2)}}
$$</span><br /></p>
<p>Through this process, we generat a total of <span class="math inline"><em>N</em>â€…Ã—â€…(<em>T</em>â€…âˆ’â€…<em>W</em>)</span> window instances, which are used as the input for clustering.</p>
<h2 id="clustering-details">Clustering Details</h2>
<p>We implemented 5 different clustering algorithms as part of the dFNC pipeline: K-Means, DBSCAN, Gaussian Mixture Models, Bayesian Gaussian Mixture Models, and Agglomerative Hierarchical Clustering.</p>
<h3 id="elbow-criterion">Elbow Criterion</h3>
<p>For K-Means, GMM, bGMM, and Agglomerative clustering, we measured the elbow criterion on a range of 2-9 components. We measured both the correlation-distance dispersion, as is recommended in Damaraju et al.Â 2014 [], as well as the silhouette measure. The results from this analysis are included below. In general, we found either unclear or multiple elbows in the range <span class="math inline"><em>K</em>â€„=â€„3,â€†4,â€†5,â€†6</span> for each data set, with the location of the elbow varying depending on the clustering algorithm used and the data set. Thus, we decided to compromise, and use the choice of <span class="math inline"><em>K</em>â€„=â€„5</span> from Damaraju et al., which is an established result for the Fbirn data set, and which is a common number of clusters chosen elswhere in the literature.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Gaussian</th>
<th style="text-align: center;">Fbirn</th>
<th style="text-align: center;">UCLA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><img width="100%" src="results/kmeans_gauss_betas/kmeans_gauss_elbow.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="results/kmeans_fbirn_betas/kmeans_fbirn_elbow.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="results/kmeans_ucla_betas/kmeans_ucla_elbow.png?raw=True" /></td>
</tr>
<tr class="even">
<td style="text-align: center;"><img width="100%" src="results/gmm_gauss_betas/gmm_gauss_elbow.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="results/gmm_fbirn_betas/gmm_fbirn_elbow.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="results/gmm_ucla_betas/gmm_ucla_elbow.png?raw=True" /></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><img width="100%" src="results/bgmm_gauss_betas/bgmm_gauss_elbow.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="results/bgmm_fbirn_betas/bgmm_fbirn_elbow.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="results/bgmm_ucla_betas/bgmm_ucla_elbow.png?raw=True" /></td>
</tr>
<tr class="even">
<td style="text-align: center;"><img width="100%" src="results/hierarchical_gauss_betas/hierarchical_gauss_elbow.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="results/hierarchical_fbirn_betas/hierarchical_fbirn_elbow.png?raw=True" /></td>
<td style="text-align: center;"><img width="100%" src="results/hierarchical_ucla_betas/hierarchical_ucla_elbow.png?raw=True" /></td>
</tr>
</tbody>
</table>
<h2 id="classification-details">Classification Details</h2>
<h3 id="feature-generation">Feature Generation</h3>
<p>As features for generation, we follow the precedent in [], and compute means of clusters for each class, and use these centers to form a regression matrix of size <span class="math inline">2<em>k</em>â€…Ã—â€…(<em>T</em>â€…âˆ’â€…<em>W</em>)</span>. For each subject, we then regress out these cluster centers from each FNC window, and collect the beta coefficients. The mean of the <span class="math inline"><em>Î²</em></span>-coefficients over all timepoints are then used as features for the final classification.</p>
<h3 id="evaluation-metric">Evaluation Metric</h3>
<p>For our evaluation metric, we used the Area-Under the â€œReceiving Operating Charactericâ€ (ROC) curve, or AUC.</p>
<h3 id="grid-search">Grid Search</h3>
<p>We performed a Grid-Search over all available parameters for each of the classifiers available in Polyssifier. For each set of training data used for cross-validation, we trained the grid-search separately, and took the parameters with the highest AUC.</p>
<h1 id="section-iv-results-discussion">Section IV: Results &amp; Discussion</h1>
<h2 id="initial-ucla-significant-comparisons">Initial UCLA Significant Comparisons</h2>
<p>In order to evaluate the predictive capacities of the features produced by each clustering algorithm, a two-tailed t-test was performed comparing the healthy control and the schizophrenic patients.</p>
<p>The first t-test comparision was performed using the cluster assignments for each patient across the time domain where each time slot represented a feature of the training data. For each time slot (feature), the average cluster assignment for all the healthy control patients was compared to the average cluster assignment for all the schizophrenic patients. The corresponding p-values were then tested at a significance level of 0.10.</p>
<p>The results displayed below highlight the points in time when there was less that a 10% chance that the observed difference in a healthy controlâ€™s cluster assignment and a schizophrenicâ€™s cluster assignment was due to normal random variation. In theory, the more points of significance across time the more likely a trained model would accurately diagnose a subject. The results indicated that both K-Means and Gaussian Mixture Models failed to produce statistically different cluster assignments across time. The Bayesian Gaussian Mixture Model produced some significant differences while the Hierarchical clustering was significant at every time point.</p>
<p>These results initially suggested that Hierarchical clustering would outperform all the other clustering algorithms, but the subsequent testing disporved this hypothesis.</p>
<p><img src="images/assignment_t_test_visualization.png?raw=true" /></p>
<p>Given the lack of improvement in accuracy across all clustering algorithms, it was believed that training supervised models using time points as features would require much more data for successful classification. Using time slots as features meant that there were 130 features in the data. Since there were only 267 patients in the UCLA data set, it was surmised that the dimensionality of the data was too high. To reduce the dimensionality of the datasets, the beta coefficients were calculated reducing the number of training features form 130 to 10.</p>
<p><img src="images/beta_t_test_visualization.png?raw=true" /></p>
<p>Each clustering algorithm produced statistically different beta coefficients between the health control and schizophrenic patients. With the reduced dimenstionality of the data, we successfully improved the accuracy of out diagnoses across all supervised learning algorithms for each clustering algorithm. These results are discussed later in the report.</p>
<h2 id="reporting-results">Reporting Results</h2>
<p>For all results, the Area Under Curve (AUC) metric was used since we were trying to classify two possible outcomes. Patients were with either healthy or had schizophrenia. AUC scores near 0.50 meant that the models randomly diagnosed patients as healthy or schizophrenic. This represented the worst possible outcome. Any model with and AUC scores between 0.40 and 0.60 was deemed a failed model. AUC scores near 1.0 meant that diagnoses were accurate with few false positives (high specificiy) and few false negatives (high recall). There were no observed models that completely reversed the diagnoses with AUC scores near 0.0.</p>
<h2 id="gaussian-simulated-dataset-1">Gaussian Simulated Dataset</h2>
<p>We initially trained the supervised learning algorithms using simulated Gaussian datasets since we did not have access to the real patient data until later in the semester. We first trained the supervised learning models without any clustering in order to establish a baseline AUC scores for our clustering algorithms to impove upon. These baseline results are displayed below.</p>
<p><img src="images/sim_pre_clustering_AUC.png?raw=true" /></p>
<p>The key result from this experiment was that without any clustering, no supervised learning algorithm could consistently achieve AUC score above 0.60. Some learners such as the Multilayer Perceptron, Passive Aggressive Classifier, and Bernoulli Naive Bayes surprisingly scored below 0.40 suggesting that they consistently misdiagnose patients. This result was attributed to being due to random error, and was disregarded.</p>
<p>After establishing the baseline AUC scores, we performed clustering and beta feature generation for the simulated Gaussian datasets. Using these clustered and reduced datasets, we trained across the same classifiers to for each clustering algorithm (K-Means, Gaussian Mixture Model, Bayesian Gaussian Mixture Model, DBSCAN, and Hierarchical) to generate the results below.</p>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 10%" />
<col style="width: 9%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 9%" />
<col style="width: 11%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="header">
<th>Clustering Algorithm</th>
<th>Multilayer Perceptron</th>
<th>Nearest Neighbors</th>
<th>SVM</th>
<th>Random Forest</th>
<th>Extra Trees</th>
<th>Gradient Boost</th>
<th>Logistic Regression</th>
<th>Passive Aggressive Classifier</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>kmeans</td>
<td>0.972 Â± 0.026</td>
<td>0.96 Â± 0.021</td>
<td>0.972 Â± 0.023</td>
<td>0.962 Â± 0.027</td>
<td>0.966 Â± 0.024</td>
<td>0.954 Â± 0.031</td>
<td>0.971 Â± 0.022</td>
<td>0.974 Â± 0.02</td>
</tr>
<tr class="even">
<td>gmm</td>
<td>0.963 Â± 0.028</td>
<td>0.954 Â± 0.03</td>
<td>0.972 Â± 0.025</td>
<td>0.967 Â± 0.022</td>
<td>0.976 Â± 0.017</td>
<td>0.928 Â± 0.046</td>
<td>0.962 Â± 0.028</td>
<td>0.97 Â± 0.024</td>
</tr>
<tr class="odd">
<td>bgmm</td>
<td>0.966 Â± 0.023</td>
<td>0.949 Â± 0.028</td>
<td><em>0.974 Â± 0.027</em></td>
<td>0.966 Â± 0.026</td>
<td>0.963 Â± 0.028</td>
<td>0.962 Â± 0.03</td>
<td>0.974 Â± 0.02</td>
<td>0.972 Â± 0.024</td>
</tr>
<tr class="even">
<td>dbscan</td>
<td>0.971 Â± 0.024</td>
<td>0.952 Â± 0.025</td>
<td>0.972 Â± 0.022</td>
<td>0.961 Â± 0.024</td>
<td>0.965 Â± 0.023</td>
<td>0.962 Â± 0.022</td>
<td>0.967 Â± 0.025</td>
<td>0.968 Â± 0.026</td>
</tr>
<tr class="odd">
<td>hierarchical</td>
<td><strong>1.0 Â± 0.0</strong></td>
<td><strong>1.0 Â± 0.0</strong></td>
<td><strong>1.0 Â± 0.0</strong></td>
<td><strong>1.0 Â± 0.0</strong></td>
<td><strong>1.0 Â± 0.0</strong></td>
<td><strong>1.0 Â± 0.001</strong></td>
<td><strong>1.0 Â± 0.0</strong></td>
<td><strong>1.0 Â± 0.0</strong></td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 10%" />
<col style="width: 11%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 15%" />
<col style="width: 12%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="header">
<th>Clustering Algorithm</th>
<th>Perceptron</th>
<th>Gaussian Process</th>
<th>Ada Boost</th>
<th>Voting</th>
<th>Bernoulli Naive Bayes</th>
<th>Bagging</th>
<th>Decision Tree</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>kmeans</td>
<td><em>0.947 Â± 0.062</em></td>
<td>0.955 Â± 0.027</td>
<td>0.957 Â± 0.032</td>
<td>0.92 Â± 0.037</td>
<td>0.948 Â± 0.022</td>
<td>0.941 Â± 0.035</td>
<td><em>0.938 Â± 0.036</em></td>
</tr>
<tr class="even">
<td>gmm</td>
<td>0.962 Â± 0.028</td>
<td>0.952 Â± 0.029</td>
<td>0.955 Â± 0.031</td>
<td>0.92 Â± 0.045</td>
<td>0.938 Â± 0.028</td>
<td>0.917 Â± 0.036</td>
<td>0.923 Â± 0.033</td>
</tr>
<tr class="odd">
<td>bgmm</td>
<td>0.969 Â± 0.029</td>
<td>0.955 Â± 0.029</td>
<td>0.958 Â± 0.029</td>
<td>0.923 Â± 0.045</td>
<td>0.949 Â± 0.028</td>
<td>0.931 Â± 0.043</td>
<td>0.933 Â± 0.026</td>
</tr>
<tr class="even">
<td>dbscan</td>
<td>0.957 Â± 0.035</td>
<td>0.96 Â± 0.032</td>
<td>0.967 Â± 0.028</td>
<td>0.913 Â± 0.034</td>
<td>0.93 Â± 0.031</td>
<td>0.93 Â± 0.036</td>
<td>0.943 Â± 0.022</td>
</tr>
<tr class="odd">
<td>hierarchical</td>
<td><strong>1.0 Â± 0.0</strong></td>
<td><strong>1.0 Â± 0.001</strong></td>
<td><strong>0.999 Â± 0.002</strong></td>
<td><strong>0.993 Â± 0.014</strong></td>
<td><strong>0.991 Â± 0.009</strong></td>
<td><strong>0.983 Â± 0.016</strong></td>
<td><strong>0.969 Â± 0.035</strong></td>
</tr>
</tbody>
</table>
<p><img width="86%" src="results/gauss_betas_accuracy.png?raw=True" /> <img width="12%" src="results/accuracy_legend.png?raw=true" /></p>
<p>The clustering combined with the beta feature generation dramatically improved the AUC scores on the simulated Gaussian datasets. All clustering algorithms produced AUC scores above 0.90 with standard deviations below 0.05 across all supervised models. The hierarchical clustering even produced perfect predictions. These results confirmed our suspicions that in order to accurately diagnose patients, we needed to perform clustering and reduce the number of features we trained our models on. These initial results using simulated data helped us tremendously when deploying our models on real patient data.</p>
<h2 id="simulated-visualizations">Simulated Visualizations</h2>
<p><img width="98%" src="results/kmeans_fbirn_betas/kmeans_fbirn_states.png?raw=true" /> Visualization of states with KMeans clustering with Gaussian Simulated Data</p>
<p><img width="49%" src="results/kmeans_gauss_betas/kmeans_gauss_visualization_3d.png?raw=true" /> <img width="49%" src="results/kmeans_gauss_betas/kmeans_gauss_visualization.png?raw=true" /><br />
Visualization of clusters with KMeans clustering in 2-d and 3-d with Gaussian Simulated Data</p>
<p><img width="98%" src="results/gmm_gauss_betas/gmm_gauss_states.png?raw=true" /> Visualization of states with GMM clustering with Gaussian Simulated Data</p>
<p><img width="49%" src="results/gmm_gauss_betas/gmm_gauss_visualization_3d.png?raw=true" /> <img width="49%" src="results/gmm_gauss_betas/gmm_gauss_visualization.png?raw=true" /><br />
Visualization of clusters with GMM clustering in 2-d and 3-d with Gaussian Simulated Data</p>
<p><img width="98%" src="results/bgmm_gauss_betas/bgmm_gauss_states.png?raw=true" /> Visualization of states with BGMM clustering with Gaussian Simulated Data</p>
<p><img width="49%" src="results/bgmm_gauss_betas/bgmm_gauss_visualization_3d.png?raw=true" /> <img width="49%" src="results/bgmm_gauss_betas/bgmm_gauss_visualization.png?raw=true" /><br />
Visualization of clusters with BGMM clustering in 2-d and 3-d with Gaussian Simulated Data</p>
<p><img width="98%" src="results/dbscan_gauss_betas/dbscan_gauss_states.png?raw=true" /> Visualization of states with DBSCAN clustering with Gaussian Simulated Data</p>
<p><img width="49%" src="results/dbscan_gauss_betas/dbscan_gauss_visualization_3d.png?raw=true" /> <img width="49%" src="results/dbscan_gauss_betas/dbscan_gauss_visualization.png?raw=true" /><br />
Visualization of clusters with DBSCAN clustering in 2-d and 3-d with Gaussian Simulated Data</p>
<p><img width="98%" src="results/hierarchical_gauss_betas/hierarchical_gauss_states.png?raw=true" /> Visualization of states with Hierarchical clustering with Gaussian Simulated Data</p>
<p><img width="49%" src="results/hierarchical_gauss_betas/hierarchical_gauss_visualization_3d.png?raw=true" /> <img width="49%" src="results/hierarchical_gauss_betas/hierarchical_gauss_visualization.png?raw=true" /><br />
Visualization of clusters with Hierarchical clustering in 2-d and 3-d with Gaussian Simulated Data</p>
<h2 id="fbirn-dataset-1">FBIRN Dataset</h2>
<p>The FBIRN dataset was trained in the exact same manner as the simulated Guassian dataset. First, the supervised models were trained without clustering or beta feature generation. As expected, the classifiers failed to consistently achieve AUC scores outside of the 0.40-0.60 range suggesting that all the models produced random diagnoses. The baseline results are displayed below.</p>
<p><img src="images/fbirn_pre_clustering_AUC.png?raw=true" /></p>
<p>Next we trained the models on the FBIRN data after clustering and using only cluster assignments. No beta features were generated to reduce the dimensionality of the training dataset. The results are displayed below.</p>
<p><img width="86%" src="results/fbirn_assignments_accuracy.png?raw=true" /> <img width="12%" src="results/accuracy_legend.png?raw=true" /></p>
<p>The results indicated that is was possible for some of the clustering algorithms to lift the AUC scores to an average of 0.70 for most supervised models with the exclusion the Voting learner. The one exception was DBSCAN which failed to move the AUC from 0.50. Regardless, none of these models achieved a high enough score to be used in a clinical environment. It was surmised the number of features in the datasets had to be reduced from cluster assignments over time to beta features in order for the supervised learning models to accurately diagnose patients.</p>
<p>After performing beta features generation with clustering, the final results for the FBIRN dataset were obtained and are displayed below.</p>
<table style="width:100%;">
<colgroup>
<col style="width: 12%" />
<col style="width: 10%" />
<col style="width: 13%" />
<col style="width: 12%" />
<col style="width: 18%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="header">
<th>Clustering Algorithm</th>
<th>SVM</th>
<th>Multilayer Perceptron</th>
<th>Logistic Regression</th>
<th>Passive Aggressive Classifier</th>
<th>Perceptron</th>
<th>Random Forest</th>
<th>Extra Trees</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>kmeans</td>
<td><em>0.952 Â± 0.036</em></td>
<td><em>0.92 Â± 0.065</em></td>
<td><em>0.944 Â± 0.039</em></td>
<td><em>0.945 Â± 0.035</em></td>
<td><strong>0.902 Â± 0.043</strong></td>
<td><em>0.871 Â± 0.038</em></td>
<td><em>0.853 Â± 0.04</em></td>
</tr>
<tr class="even">
<td>gmm</td>
<td><em>0.936 Â± 0.054</em></td>
<td><em>0.946 Â± 0.038</em></td>
<td><em>0.943 Â± 0.038</em></td>
<td><em>0.929 Â± 0.031</em></td>
<td><em>0.882 Â± 0.04</em></td>
<td><strong>0.885 Â± 0.022</strong></td>
<td><strong>0.874 Â± 0.026</strong></td>
</tr>
<tr class="odd">
<td>bgmm</td>
<td><em>0.955 Â± 0.037</em></td>
<td><em>0.932 Â± 0.042</em></td>
<td><em>0.945 Â± 0.038</em></td>
<td><em>0.939 Â± 0.038</em></td>
<td><em>0.896 Â± 0.074</em></td>
<td><em>0.86 Â± 0.039</em></td>
<td><em>0.87 Â± 0.056</em></td>
</tr>
<tr class="even">
<td>dbscan</td>
<td>0.883 Â± 0.027</td>
<td>0.893 Â± 0.031</td>
<td>0.892 Â± 0.033</td>
<td>0.884 Â± 0.027</td>
<td>0.828 Â± 0.064</td>
<td>0.805 Â± 0.064</td>
<td>0.806 Â± 0.058</td>
</tr>
<tr class="odd">
<td>hierarchical</td>
<td><strong>0.957 Â± 0.032</strong></td>
<td><strong>0.954 Â± 0.038</strong></td>
<td><strong>0.953 Â± 0.038</strong></td>
<td><strong>0.951 Â± 0.032</strong></td>
<td><em>0.891 Â± 0.098</em></td>
<td><em>0.881 Â± 0.032</em></td>
<td><em>0.872 Â± 0.048</em></td>
</tr>
</tbody>
</table>
<p><img width="86%" src="results/fbirn_betas_accuracy.png?raw=True" /> <img width="12%" src="results/accuracy_legend.png?raw=true" /></p>
<p>As expected, the reduced number of training features increased the accuracy of all the learners with Support Vector Machines acheiveing the highest accuracy across all clustering algorithms. Only DBCAN failed to get above an average AUC score of 0.90 for support vector machines and Hierarchical clusterting obtained this highest score of 0.957.</p>
<h2 id="fbirn-data-visualizations">FBIRN Data Visualizations</h2>
<p><img width="98%" src="results/kmeans_fbirn_betas/kmeans_fbirn_states.png?raw=true" /> Visualization of states with KMeans clustering with FBirn Data</p>
<p><img width="49%" src="results/kmeans_fbirn_betas/kmeans_fbirn_visualization_3d.png?raw=true" /> <img width="49%" src="results/kmeans_fbirn_betas/kmeans_fbirn_visualization.png?raw=true" /><br />
Visualization of clusters with KMeans clustering in 2-d and 3-d with FBirn Data</p>
<p><img width="98%" src="results/gmm_fbirn_betas/gmm_fbirn_states.png?raw=true" /> Visualization of states with GMM clustering with FBirn Data</p>
<p><img width="49%" src="results/gmm_fbirn_betas/gmm_fbirn_visualization_3d.png?raw=true" /> <img width="49%" src="results/gmm_fbirn_betas/gmm_fbirn_visualization.png?raw=true" /><br />
Visualization of clusters with GMM clustering in 2-d and 3-d with FBirn Data</p>
<p><img width="98%" src="results/bgmm_fbirn_betas/bgmm_fbirn_states.png?raw=true" /> Visualization of states with BGMM clustering with FBirn Data</p>
<p><img width="49%" src="results/bgmm_fbirn_betas/bgmm_fbirn_visualization_3d.png?raw=true" /> <img width="49%" src="results/bgmm_fbirn_betas/bgmm_fbirn_visualization.png?raw=true" /><br />
Visualization of clusters with BGMM clustering in 2-d and 3-d with FBirn Data</p>
<p><img width="98%" src="results/dbscan_fbirn_betas/dbscan_fbirn_states.png?raw=true" /> Visualization of states with DBSCAN clustering with FBirn Data</p>
<p><img width="49%" src="results/dbscan_fbirn_betas/dbscan_fbirn_visualization_3d.png?raw=true" /> <img width="49%" src="results/dbscan_fbirn_betas/dbscan_fbirn_visualization.png?raw=true" /><br />
Visualization of clusters with DBSCAN clustering in 2-d and 3-d with FBirn Data</p>
<p><img width="98%" src="results/hierarchical_fbirn_betas/hierarchical_fbirn_states.png?raw=true" /> Visualization of states with Hierarchical clustering with FBirn Data</p>
<p><img width="49%" src="results/hierarchical_fbirn_betas/hierarchical_fbirn_visualization_3d.png?raw=true" /> <img width="49%" src="results/hierarchical_fbirn_betas/hierarchical_fbirn_visualization.png?raw=true" /><br />
Visualization of clusters with Hierarchical clustering in 2-d and 3-d with FBirn Data</p>
<h2 id="ucla-dataset-1">UCLA Dataset</h2>
<p>The UCAL Dataset was similarly trained. Firstly, it was trained using unclustered data to establish a baseline to compare against. The results are displayed below and as expected the supervised models failed to obtain suitable AUC scores.</p>
<p><img src="images/ucla_pre_clustering_AUC.png?raw=true" /></p>
<p>The below plot shows â€œAccuracyâ€ of various classifiers such as KMeans, Gaussian Mixture Model(GMM), Bayesian Gaussian Mixture Model(BGMM), Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and Hierarchical clustering methods on Simulated Gaussian Data using beta features. Accuracy has improved a lot in comparison to the previous case above without clusterer and without using beta feature say for example from 0.7 to 0.9 for KMeans with â€œMulti Layer Perceptronâ€ and likewise for other clusterer. Almost all the classifiers in all the clusterer shows improvement over KMeans clustering.</p>
<table style="width:100%;">
<colgroup>
<col style="width: 12%" />
<col style="width: 10%" />
<col style="width: 13%" />
<col style="width: 12%" />
<col style="width: 18%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="header">
<th>Clustering Algorithm</th>
<th>SVM</th>
<th>Multilayer Perceptron</th>
<th>Logistic Regression</th>
<th>Passive Aggressive Classifier</th>
<th>Perceptron</th>
<th>Extra Trees</th>
<th>Random Forest</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>kmeans</td>
<td><em>0.907 Â± 0.057</em></td>
<td><em>0.907 Â± 0.057</em></td>
<td><em>0.904 Â± 0.06</em></td>
<td><strong>0.896 Â± 0.08</strong></td>
<td><em>0.799 Â± 0.116</em></td>
<td><em>0.724 Â± 0.168</em></td>
<td><em>0.746 Â± 0.133</em></td>
</tr>
<tr class="even">
<td>gmm</td>
<td><strong>0.91 Â± 0.059</strong></td>
<td><strong>0.909 Â± 0.07</strong></td>
<td><strong>0.908 Â± 0.071</strong></td>
<td><em>0.885 Â± 0.087</em></td>
<td><strong>0.886 Â± 0.058</strong></td>
<td><em>0.795 Â± 0.095</em></td>
<td><em>0.785 Â± 0.108</em></td>
</tr>
<tr class="odd">
<td>bgmm</td>
<td><em>0.909 Â± 0.075</em></td>
<td><em>0.907 Â± 0.081</em></td>
<td><strong>0.908 Â± 0.08</strong></td>
<td><em>0.877 Â± 0.105</em></td>
<td><em>0.879 Â± 0.081</em></td>
<td><em>0.741 Â± 0.157</em></td>
<td><em>0.705 Â± 0.166</em></td>
</tr>
<tr class="even">
<td>dbscan</td>
<td>0.409 Â± 0.118</td>
<td>0.467 Â± 0.131</td>
<td>0.69 Â± 0.096</td>
<td>0.667 Â± 0.122</td>
<td>0.5 Â± 0.0</td>
<td>0.643 Â± 0.171</td>
<td>0.649 Â± 0.125</td>
</tr>
<tr class="odd">
<td>hierarchical</td>
<td><em>0.886 Â± 0.054</em></td>
<td><em>0.889 Â± 0.07</em></td>
<td><em>0.9 Â± 0.069</em></td>
<td><em>0.883 Â± 0.071</em></td>
<td><em>0.826 Â± 0.122</em></td>
<td><strong>0.829 Â± 0.099</strong></td>
<td><strong>0.792 Â± 0.114</strong></td>
</tr>
</tbody>
</table>
<p><img width="86%" src="results/ucla_betas_accuracy.png?raw=True" /> <img width="12%" src="results/accuracy_legend.png?raw=true" /></p>
<h2 id="ucla-data-visualisations">UCLA Data Visualisations</h2>
<p><img width="98%" src="results/kmeans_ucla_betas/kmeans_ucla_states.png?raw=true" /> Visualization of states with KMeans clustering with UCLA Data</p>
<p><img width="49%" src="results/kmeans_ucla_betas/kmeans_ucla_visualization_3d.png?raw=true" /> <img width="49%" src="results/kmeans_ucla_betas/kmeans_ucla_visualization.png?raw=true" /><br />
Visualization of clusters with KMeans clustering in 2-d and 3-d with UCLA Data</p>
<p><img width="98%" src="results/gmm_ucla_betas/gmm_ucla_states.png?raw=true" /> Visualization of states with GMM clustering with UCLA Data</p>
<p><img width="49%" src="results/gmm_ucla_betas/gmm_ucla_visualization_3d.png?raw=true" /> <img width="49%" src="results/gmm_ucla_betas/gmm_ucla_visualization.png?raw=true" /><br />
Visualization of clusters with GMM clustering in 2-d and 3-d with UCLA Data</p>
<p><img width="98%" src="results/bgmm_ucla_betas/bgmm_ucla_states.png?raw=true" /> Visualization of states with BGMM clustering with UCLA Data</p>
<p><img width="49%" src="results/bgmm_ucla_betas/bgmm_ucla_visualization_3d.png?raw=true" /> <img width="49%" src="results/bgmm_ucla_betas/bgmm_ucla_visualization.png?raw=true" /><br />
Visualization of clusters with BGMM clustering in 2-d and 3-d with UCLA Data</p>
<p><img width="98%" src="results/dbscan_ucla_betas/dbscan_ucla_states.png?raw=true" /> Visualization of states with DBSCAN clustering with UCLA Data</p>
<p><img width="49%" src="results/dbscan_ucla_betas/dbscan_ucla_visualization_3d.png?raw=true" /> <img width="49%" src="results/dbscan_ucla_betas/dbscan_ucla_visualization.png?raw=true" /><br />
Visualization of clusters with DBSCAN clustering in 2-d and 3-d with UCLA Data</p>
<p><img width="98%" src="results/hierarchical_ucla_betas/hierarchical_ucla_states.png?raw=true" /> Visualization of states with Hierarchical clustering with UCLA Data</p>
<p><img width="49%" src="results/hierarchical_ucla_betas/hierarchical_ucla_visualization_3d.png?raw=true" /> <img width="49%" src="results/hierarchical_ucla_betas/hierarchical_ucla_visualization.png?raw=true" /><br />
Visualization of clusters with Hierarchical clustering in 2-d and 3-d with UCLA Data</p>
<h1 id="section-v-conclusion">Section V: Conclusion</h1>
<h1 id="dfncluster">dFNCluster</h1>
<p>dFNCluster implements Dynamic Functional Network Connectivity (dFNC) with several clustering algorithms, and actively compares the performance of classification under different clustering algorithms and hyper-parameters.</p>
<h2 id="prerequisites">Prerequisites</h2>
<p>First, install git submodules</p>
<pre><code>git submodule update --init --recursive</code></pre>
<p>This project has been tested in Python 3.6+</p>
<p>It is recommended you use a conda virtual environment.</p>
<pre><code>conda create -y --name dfncluster</code></pre>
<p>and install requirements via pip</p>
<pre><code>pip install -r requirements.txt</code></pre>
<h2 id="running-the-code">Running the Code</h2>
<p>You can run <code>main.py</code> with the arguments given below, or look at them by running <code>python main.py --help</code></p>
<pre><code>usage: main.py [-h] [--dataset DATASET] [--remake_data REMAKE_DATA]
               [--clusterer CLUSTERER] [--window_size WINDOW_SIZE]
               [--time_index TIME_INDEX] [--clusterer_params CLUSTERER_PARAMS]
               [--classifier_params CLASSIFIER_PARAMS] [--outdir OUTDIR]
               [--dfnc DFNC] [--classify CLASSIFY] [--subset_size SUBSET_SIZE]
               [--dfnc_outfile DFNC_OUTFILE] [--seed SEED] [--k K]

optional arguments:
  -h, --help            show this help message and exit
  --dataset DATASET     &lt;str&gt; the data set to use. Options are fbirn, simtb,
                        gaussian; DEFAULT=fbirn
  --remake_data REMAKE_DATA
                        &lt;bool&gt; whether or not to remake the data set;
                        DEFAULT=False
  --clusterer CLUSTERER
                        &lt;str&gt; the clusterer to use. Options are kmeans, bgmm,
                        gmm, dbscan; DEFAULT=kmeans
  --window_size WINDOW_SIZE
                        &lt;int&gt; the size of the dFNC window; DEFAULT=22
  --time_index TIME_INDEX
                        &lt;int&gt; the dimension in which dFNC windows will be
                        computed; DEFAULT=1
  --clusterer_params CLUSTERER_PARAMS
                        &lt;str(dict)&gt; dict to be loaded for classifier
                        params(JSON); DEFAULT=&quot;{}&quot;
  --classifier_params CLASSIFIER_PARAMS
                        &lt;str(dict)&gt; dict to be loaded for classifier params
                        (JSON); DEFAULT=&quot;{}&quot;
  --outdir OUTDIR       &lt;str&gt; Name of the results directory. Saving hierarchy
                        is: results/&lt;outdir&gt;; DEFAULT=FNCOnly
  --dfnc DFNC           &lt;bool&gt; Do or do not run dFNC; DEFAULT=True
  --classify CLASSIFY   &lt;bool&gt; Do or do not do classification; DEFAULT=True
  --subset_size SUBSET_SIZE
                        &lt;float [0,1]&gt; percentage of data to use; DEFAULT=1.0
                        (all data)
  --dfnc_outfile DFNC_OUTFILE
                        &lt;str&gt; The filename for saving dFNC results;
                        DEFAULT=dfnc.npy
  --seed SEED           &lt;int&gt; Seed for numpy RNG. Used for random generation
                        of the data set, or for controlling randomness in
                        Clusterings.; DEFAULT=None (do not use seed)
  --k K                 &lt;int&gt; number of folds for k-fold cross-validation</code></pre>
<h2 id="examples">Examples</h2>
<h2 id="sklearn-datasets">Sklearn Datasets</h2>
<p>To generate a data set from SKlearn for testing purposes, you can generate one of the datasets in <code>data/SklearnDatasets</code>.</p>
<p>For example, the moons data set can be generated as follows:</p>
<pre><code>PYTHONPATH=. python data/SklearnDatasets/Moons/Moons.py</code></pre>
<p>which will save <code>moons.npy</code> in the <code>data/SklearnDatasets/Moons</code> directory.</p>
<p>The following datasets have been included as examples:</p>
<ul>
<li>Moons</li>
<li>Classification (sklearn.datasets.make_classification)</li>
<li>Blobs (sklearn.datasets.make_blobs)</li>
<li>MNIST (sklearn.datasets.fetch_openml(name=â€™mnist_â€¦)</li>
<li>Iris (sklearn.datasets.load_iris)</li>
</ul>
<h2 id="fbirn-data">Fbirn Data</h2>
<p>To run with the pre-computed ICA Timecourses from real data, run the following</p>
<p>first, untar the data</p>
<pre><code>cd data/MatDatasets/FbirnTC
tar -xzf subjects.tar.gz</code></pre>
<p>back in the dfncluster directory, build the data set, which serializes the data set object as a pickled npy file</p>
<pre><code>PYTHONPATH=. python data/MatDatasets/FbirnTC/FbirnTC.py</code></pre>
<p>And run the main function, which performs dFNC analysis, and classification using cluster assignments, and raw states for comparison</p>
<pre><code>PYTHONPATH=. python main.py</code></pre>
<h2 id="simtb-data">SIMTB Data</h2>
<p>To run with the pre-computed ICA Timecourses, run the following</p>
<p>first, untar the simulated data</p>
<pre><code>cd data/MatDatasets/OmegaSim
tar -xzf subjects.tar.gz</code></pre>
<p>back in the dfncluster directory, build the simulations data set, which serializes the data set object as a pickled npy file</p>
<pre><code>PYTHONPATH=. python data/MatDatasets/OmegaSim/OmegaSim.py</code></pre>
<h3 id="running-on-subsets">Running on Subsets</h3>
<p>To run on a subset of the simulated data set, you can either edit data.csv in the data directory, and rebuild, or copy that directory under a new name, edit, rebuild and point main.py to the new data set.</p>
<h2 id="references">References</h2>
<p><a name="ref1"></a> [1]. Eswar Damaraju et al. â€œDynamic functional connectivity analysis reveals transient states of dyscon-nectivity in schizophreniaâ€. In:NeuroImage: Clinical5 (2014), pp.Â 298â€“308.<br></p>
<p><a name="ref2"></a> [2]. Mustafa S Salman et al. â€œGroup ICA for identifying biomarkers in schizophrenia:â€˜Adaptiveâ€™networks viaspatially constrained ICA show more sensitivity to group differences than spatio-temporal regressionâ€.In:NeuroImage: Clinical22 (2019), p.Â 101747.<br></p>
<p><a name="ref3"></a> [3]. Elena A Allen et al. â€œTracking whole-brain connectivity dynamics in the resting stateâ€. In:Cerebralcortex24.3 (2014), pp.Â 663â€“676.<br></p>
<p><a name="ref4"></a> [4]. D. Zhi et al., â€œAbnormal Dynamic Functional Network Connectivity and Graph Theoretical Analysis in Major Depressive Disorder,â€ 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Honolulu, HI, 2018, pp.Â 558-561.<br></p>
<p><a name="ref5"></a> [5]. U Sakoglu, AM Michael, and VD Calhoun. â€œClassification of schizophrenia patients vs healthy controlswith dynamic functional network connectivityâ€. In:Neuroimage47.1 (2009), S39â€“41.<br></p>
<p><a name="ref6"></a> [6]. Unal Sako Ì†glu et al. â€œA method for evaluating dynamic functional network connectivity and task-modulation: application to schizophreniaâ€. In:Magnetic Resonance Materials in Physics, Biology andMedicine23.5-6 (2010), pp.Â 351â€“366.<br></p>
<p><a name="ref7"></a> [7]. V. M. Vergara, A. Abrol, F. A. Espinoza and V. D. Calhoun, &quot;Selection of Efficient Clustering Index to Estimate the Number of Dynamic Brain States from Functional Network Connectivity*,&quot; 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Berlin, Germany, 2019, pp.Â 632-635.<br></p>
<p><a name="ref8"></a> [8]. D. K. Saha, A. Abrol, E. Damaraju, B. Rashid, S. M. Plis and V. D. Calhoun, â€œClassification As a Criterion to Select Model Order For Dynamic Functional Connectivity States in Rest-fMRI Data,â€ 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019), Venice, Italy, 2019, pp.Â 1602-1605.<br></p>
<p><a name="ref9"></a> [9]. Pedregosa et al. â€œ2.3. Clustering.â€ Scikit, scikit-learn.org/stable/modules/clustering.html.<br></p>
<p><a name="ref10"></a> [10]. Rashid, Barnaly, et al. â€œClassification of Schizophrenia and Bipolar Patients Using Static and Dynamic Resting-State FMRI Brain Connectivity.â€ NeuroImage, vol.Â 134, 2016, pp.Â 645â€“657., doi:10.1016/j.neuroimage.2016.04.051.</p>
</body>
</html>

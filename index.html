<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Cluster Analysis in Dynamic Functional Network Connectivity</title>
  <style type="text/css">
    code {
      white-space: pre-wrap;
    }

    span.smallcaps {
      font-variant: small-caps;
    }

    span.underline {
      text-decoration: underline;
    }

    div.column {
      display: inline-block;
      vertical-align: top;
      width: 50%;
    }
  </style>
  <link rel="stylesheet" href="github-markdown.css">

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      displayMath: [ ['$$','$$'] ]
    }
  });
</script>

  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>

<body>
  <div class="my-title">
    The Role of Clustering in Dynamic Functional Network Connectivity for Schizophrenia Diagnosis
  </div>
  <div class="authors">
    Bradley Baker, Dahim Choi, Anish Khazane, Jitendra Khumar, Eric Martin
  </div>
  <div class="markdown-body">
    <h1 id="introduction">Introduction</h1>
    <figure>
      <img src="results/Overview_pipeline.png" />
      <figcaption><b>Figure:</b> Overview of our contribution with this project. The standard pipeline for dFNC uses
        only KMeans clustering
        as part of the pipeline, which introduces implicit assumptions on the organization of the FNC feature-space. We
        test four other clustering
        algorithms, and evaluate how they produce FNC states, and how these states aid in schizophrenia classification.
      </figcaption>
    </figure>

    <p>Schizophrenia is a chronic and serious mental disorder which affects how a person thinks, feels, and behaves.
      Although there have been many studies about psychological and behavioral manifestations of schizophrenia,
      neuroscientists have yet to determine a set of corresponding neurological biomarkers for this disorder. A
      functional
      magnetic resonance imaging (fMRI) can help determine non-invasive biomarkers for schizophrenia in brain function<a
        href="#ref1">[1]</a><a href="#ref2">[2]</a> and one such fMRI analysis technique called, dynamic functional
      network connectivity (dFNC)<a href="#ref3">[3]</a><a href="#ref4">[4]</a><a href="#ref10">[10]</a>, uses K-Means
      clustering to characterize time-varying connectivity between functional networks. Researchers have worked on
      finding
      correlation between schizophrenia and dFNC<a href="#ref1">[1]</a><a href="#ref2">[2]</a><a href="#ref5">[5]</a><a
        href="#ref6">[6]</a>. Indeed, dFNC features are well-established as features for schizophrenia classification <a
        href="#ref14">[14]</a>.
    </p>
    <p>
      Despite the utility of dFNC analysis, little work has been done analyzing the choice of clustering algorithm,
      which in most
      applications is simply K-Means<a href="#ref7">[7]</a><a href="#ref9">[9]</a>.
      It is possible that the reliance of K-Means assumptions (sphericality of clusters, reliability of elbow-criterion,
      etc)
      have introduced bias into previous results using dFNC, and restricted both biological discovery, and reliability
      of
      classifiers used for diagnosis.</p>

    <p>
      Therefore, in this project, we have
      studied how modifying the
      clustering technique in the dFNC pipeline can yield dynamic states from fMRI data, and how that choice impacts the
      accuracy of
      classifying schizophrenia<a href="#ref8">[8]</a>.</p>

    <p>We experimented with DBSCAN, Hiearcharial Clustering, Gaussian Mixture Models, and Bayesian Gaussian Mixture
      Models
      clustering methods on subject connectivity matrices produced from fMRI data, and each algorithm’s cluster
      assignments as features for SVMs, MLP, Nearest Neighbor, and other supervised classification algorithms to
      classify
      schizophrenia.</p>
    <p>Section II describes the fMRI data used in our experimentation, while Section III summarizes the aforementioned
      clustering and classification algorithms used in the pipeline. Section IV compares the accuracy of these
      classifiers, along with presenting a series of charts that analyze the cluster assignments produced on the fMRI
      data.</p>



    <h1 id="section-ii-data">Section II: Data</h1>
    <p>All datasets used in this project are derivatives of fMRI data (functional magnetic resonance imaging), which is
      data measuring brain activity to track a patient’s thought processs over a predefined time period.</p>
    <p> The BOLD signal in fMRI data tracks the movement of blood throughout a subject's brain during the scan period.
      These
      changes in blood flow can be further analyzed to make conclusions about flow between different regions, regions of
      concentrated activity, and other conclusions, which can be useful for the diagnosis of mental disorders, among
      other
      applications. The full 4-Dimensional volumes are often not used for classification in their raw form, and instead
      derivatives
      such as Functional Network Connectivity states can be used for diagnosis.
    </p>

    <figure>
      <img src="https://i.stack.imgur.com/lkXDb.png" />
      <figcaption><b>Figure:</b> Visualization of a functional MRI time-series volume. Image from Chapter 2 of
        Lindquist et al. [24]</figcaption>
    </figure>

    <table>
      <thead>
        <tr class="header">
          <th>Data Set Name</th>
          <th>Number of Subjects</th>
          <th>Number of Healthy Controls</th>
          <th>Number of Patients</th>
        </tr>
      </thead>
      <tbody>
        <tr class="odd">
          <td><b>FBIRN</b></td>
          <td>362</td>
          <td>186</td>
          <td>176</td>
        </tr>
        <tr class="even">
          <td><b>UCLA</b></td>
          <td>332</td>
          <td>272</td>
          <td>50</td>
        </tr>
        <tr class="odd">
          <td><b>Simulations</b></td>
          <td>300</td>
          <td>150</td>
          <td>150</td>
        </tr>
      </tbody>
    </table>

    <h3 id="gaussian-simulated-dataset">Gaussian Simulated Dataset</h3>
    We generated synthetic FNC data which were used for debugging code, and evaluating
    our method. The details of the simulation are described in the appendix.
    <h3 id="fbirn-dataset">FBIRN Dataset</h3>
    <p>We use derivatives from the Phase 3 Dataset of FBIRN (Functional Biomedical Infromatics Research Network Data
      Repository), which specifically focuses on brain activity maps from patients with schizophrenia. This dataset
      includes 186 healthy controls and 176 indivduals from schizophrenia from around the United States. Subject
      participants in this dataset are between the ages of 18-62.</p>
    <h3 id="ucla-dataset">UCLA Dataset</h3>
    <p>We also use derivatives from the UCLA Consortium for Neuropsychiatric Phenomics archives, which includes
      neuroimages for roughly 272 participants. The subject population consists of roughly 272 healthy controls, as well
      as participants with a diagnosis of schizophrenia (50 subjects). Subject participants in this dataset range from
      21
      to 50 years</p>


    <h1 id="section-iii-methods">Section III: Methods</h1>
    <h2 id="overview">Overview</h2>
    <p>
      <figure>
        <img width="50%" src="results/dfnc_pipeline(1).png?raw=True" />
        <figcaption><b>Figure:</b> Pipeline overview of the full dFNC classification framework. In our analysis, we
          are interested in evaluating the role of the Clustering algorithm on the results.</figcaption>
      </figure>
    </p>
    <h2 id="preprocessing">Preprocessing</h2>
    <p>As is standard in Dynamic Functional Network Connectivity, Group Independent Component Analysis of subject images
      is used to compute a set of <span class="math inline"><em>M</em></span> statistically independent spatial maps
      from
      the data, along with a set of <span class="math inline"><em>M</em></span> time-courses, which are used for the
      dFNC
      analysis. For the Fbirn data, we used pre-computed ICA timecourses provided by Damaraju et al. 2014. These
      timecourses were computed using Infomax ICA <a href="#ref11">[11]</a> with 100 components, which were then
      manually selected based on biological relevance for a total of 47 remaining components.</p>
    <p>For the UCLA data, because
      no ICA derivatives were
      readily available, we used Group Information Guided ICA <a href="#ref12">[12]</a> which is included as part of the
      GIFT NeuroImaging
      Analysis Toolbox <a href="#ref15">[15]</a>. As a template for GIG-ICA, we used the NeuroMark template <a
        href="#ref16">[16]</a>, which has been shown to provide
      reliable estimations of components in many different settings <a href="#ref13">[13]</a>. This provides us with 53
      independent components for
      Uhe UCLA data.</p>
    <h3 id="sliding-window-analysis">Sliding Window Analysis</h3>
    <p>For all data-sets we chose a sliding window of size <span class="math inline"><em>W</em> = 22</span>, following
      the
      precedent in Damaraju et al. 2014 <a href="#ref1">[1]</a>. Time-Series data <span
        class="math inline"><em>X</em></span> with size <span class="math inline"><em>M</em> × <em>T</em></span>, where
      <span class="math inline"><em>M</em></span> is the
      number of independent components, and <span class="math inline"><em>T</em></span> is the number of timepoints are
      used. Time series were normalized and convolved with a gaussian window with a kernel of size 0.015, again
      following
      the precedent in Damaraju et al. 2014 <a href="#ref1">[1]</a>. For each window on each time-series a number of
      exemplar data-points were
      selected by taking the local maximum of the variance from the smoothed time-series.</p>
    <p>We computed correlation coefficients across the components, within each time series window to form the FNC matrix
      with entries <span class="math inline"><em>i</em></span> and
      <span class="math inline"><em>j</em></span> given as
      <figure>
        <img style="float:left;padding-right:10px;padding-bottom:10px;" src="results/centered_equation_1.png?raw=True"
          alt="Image" width="20%" />
      </figure>

      <h2 style="clear:left;"></h2>

      <!-- <img align="left" width="40%"gi src="results/centered_equation_1.png?raw=True" />
 -->
      <p>Through this process, we generate a total of <span
          class="math inline"><em>N</em> × (<em>T</em> − <em>W</em>)</span> window instances, which are used as the
        input
        for clustering.</p>
      <h2 id="clustering-details">Clustering Details</h2>
      <p>We implemented 5 different clustering algorithms as part of the dFNC pipeline:</p>
      <ul>
        <li><b>KMeans</b></li>
        <li><b>DBSCAN</b></li>
        <li><b>Gaussian Mixture Models</b></li>
        <li><b>Bayesian Gaussian Mixture Models</b></li>
        <li><b>Agglomerative Hierarchical Clustering</b></li>
      </ul>

      <p>For each of these algorithms, we used them for both exemplar clustering, and clustering on the full data. In
        the
        case of KMeans,
        Gaussian Mixture Models, and Bayesian Gaussian Mixture Models, exemplar cluster-centers were used as input for
        initialization
        in the second stage of clustering using the same algorithm. In the case of DBSCAN and Agglomerative clustering,
        the exemplar stage was only used for elbow criterion, and other hyper-parameter tuning.</p>

      <p>For GMMs and Bayesian GMMs, we elected to use hard-assignment for determining cluster membership, because it
        lent to simpler integration with other steps in the pipeline. In future work, we would like to try to use the
        more rich information provided by soft-assignment to somehow improve feature generation, which is further
        discussed below.</p>

      <h2 id="classification-details">Classification Details</h2>

      <h3>Task Formulation</h3>

      <p>Our chosen task is the binary classification of Schizophrenic Patients from Healthy Controls using
        Functional Network Connectivity states. For patients with a schizophrenic diagnosis, we assign the label
        <b>1</b>,
        and for all other subjects, we assign the label <b>0</b>. Given the set of FNC features for all subjects, our
        goal is to find the classifier which provides the most accurate diagnosis.</p>

      <h3 id="evaluation-metric">Evaluation Metric</h3>
      <p>For our evaluation metric, we used the Area-Under the “Receiving Operating Characteric” (ROC) curve, or
        <b>ROC-AUC</b>.
        This metric, which is often used for binary classifiers, describes the probability that a given classifier will
        rank a randomly chosen positive instance higher than a randomly chosen negative instance.
        The curve is creative by plotting the fraction of true positive out of the positive vs the fraction of
        false pasitive out of the negatives over various thresholds $T$. In general, a higher AUC indicates a better
        binary classification performance.

      </p>

      <h3 id="feature-generation">Feature Generation</h3>
      <p>Classification over raw dynamic FNC states is often poor, and further feature generation is required.</p>

      <p>We first experimented with the use of subject state-vectors for classification. Because each time point for
        each subject is assigned a state during clustering, we can generate a vector of $T-W$ state assignments for Each
        subject to be used as features.
      </p>

      <p>Secondly, we follow the precedent in <a href="#ref14">[14]</a>, and compute means of clusters
        for each class, and
        use
        these centers to form a regression matrix of size <span
          class="math inline">2<em>k</em> × (<em>T</em> − <em>W</em>)</span>. For each subject, we then linearly regress
        out
        these
        cluster centers from each FNC window, and collect the beta coefficients. The mean of the <span
          class="math inline"><em>β</em></span>-coefficients over all timepoints are then used as features for the final
        classification.</p>

      <p>
        We computed two-tailed T-Tests over each of the feature spaces used for classification, and also
        over the centroids for each class prior to regression, in order to visualize statistically significant
        features which may or may not improve the performance of the classifiers.
      </p>

      <h3>Chosen Algorithms</h3>

      <p>For supervised learning, we used a modified version of the <a
          href="(https://github.com/alvarouc/polyssifier">Polyssifier classification
          toolbox</a>,
        which wraps the majority of the classification algorithms from SKlearn into an end-to-end framework. The
        modified version
        of the toolbox, which is being maintained by <a href="https://github.com/trendscenter/polyssifier">Brad
          Baker</a>, . Within polyssifier, we implemented the following binary
        classifiers:
      </p>
      <p>
        <ul>
          <li>Multilayer Perceptron</li>
          <li>K-Nearest Neighbors Classifier</li>
          <li>Support Vector Machines</li>
          <li>Decision Tree Classifier</li>
          <li>Random Forests <a href="#random_forests">[17]</a></li>
          <li>Extra Trees <a href="#extra_trees">[18]</a></li>
          <li>Ada-Boost Decision Trees <a href="#adaboost">[19]</a></li>
          <li>Bagging Decision Trees <a href="#bagging">[20]</a></li>
          <li>Gradient Boosted Decision Trees <a href="#gradient_boost">[21]</a></li>
          <li>Logistic Regression</li>
          <li>Passive Aggressive Classifier <a href="#passive_aggresive">[22]</a></li>
          <li>Perceptron</li>
          <li>Naive Bayes</li>
          <li>Voting Classifer</li>
        </ul>
      </p>

      <h3 id="grid-search">Grid Search</h3>

      <p>We performed a Grid-Search over all available parameters for each of the classifiers available in Polyssifier.
        For each set of training data used for cross-validation, we trained the grid-search separately, and took the
        parameters with the highest AUC.</p>

      <p>For the sake of space, we have ommitted the details of the grid-search, but the results have been included in
        the
        results section of the repo. Particularly, [the best parameters for each classification algorithm are saved
        here]().
      </p>

      <h1 id="section-iv-results-discussion">Section IV: Results &amp; Discussion</h1>
      <hr />
      <hr />
      <h2 id="reporting-results">Classification Results</h2>
      <p>For all results, the Area Under Curve (AUC) metric was used since we were trying to classify two possible
        outcomes. Patients were with either healthy or had schizophrenia. AUC scores near 0.50 meant that the models
        randomly diagnosed patients as healthy or schizophrenic. This represented the worst possible outcome. Any model
        with and AUC scores between 0.40 and 0.60 was deemed a failed model. AUC scores near 1.0 meant that diagnoses
        were
        accurate with few false positives (high specificiy) and few false negatives (high recall). There were no
        observed
        models that completely reversed the diagnoses with AUC scores near 0.0.</p>

      <p>Each classification algorithm was evaluated with <b>10-fold cross-validation</b>, with a grid-search
        for classifier parameters performed separately on each validation set.
      </p>

      <h2 id="fbirn-dataset-1">FBIRN Dataset</h2>
      <p>The FBIRN dataset was trained in the exact same manner as the simulated Guassian dataset. First, the supervised
        models were trained without clustering or beta feature generation. As expected, the classifiers failed to
        consistently achieve AUC scores outside of the 0.40-0.60 range suggesting that all the models produced random
        diagnoses. The baseline results are displayed below.</p>
      <figure>
        <img src="images/fbirn_accuracy.png?raw=true" />
        <figcaption><b>Figure: </b>AUC scores for 10-fold cross-validation of multiple classifiers using only the
          concatenated FNC
          matrices as features. The y axis is the AUC metric for each classifier, and the x-axis separates the
          classifiers.</figcaption>
      </figure>
      <p>Next we trained the models on the FBIRN data after clustering and using only cluster assignments. No beta
        features were generated to reduce the dimensionality of the training dataset. The results are displayed below.
      </p>

      <figure>
        <img width="100%" src="results/fbirn_assignments_accuracy.png?raw=true" />
        <img style="width:30%" src="results/accuracy_legend.png?raw=true" />
        <figcaption><b>Figure: </b> Fbirn Schizophrenia Classification AUC scores using subject state-vectors as
          features. The y-axis
          provides the AUC metric
          and the x-axis separates the scores obtained over 10-fold cross-validation for each classifier. </figcaption>
      </figure>
      <p>The results indicated that is was possible for some of the clustering algorithms to lift the AUC scores to an
        average of 0.70 for most supervised models with the exclusion the Voting learner. The one exception was DBSCAN
        which failed to move the AUC from 0.50. Regardless, none of these models achieved a high enough score to be used
        in a clinical environment. It was surmised the number of features in the datasets had to be reduced from cluster
        assignments over time to beta features in order for the supervised learning models to accurately diagnose
        patients.</p>
      <p>After performing beta features generation with clustering, the final results for the FBIRN dataset were
        obtained
        and are displayed below.</p>
      <table style="width:50%;">
        <colgroup>
          <col style="width: 12%" />
          <col style="width: 10%" />
          <col style="width: 13%" />
          <col style="width: 12%" />
          <col style="width: 18%" />
          <col style="width: 10%" />
          <col style="width: 10%" />
          <col style="width: 10%" />
        </colgroup>
        <thead>
          <tr class="header">
            <th>Clustering Algorithm</th>
            <th>SVM</th>
            <th>Multilayer Perceptron</th>
            <th>Logistic Regression</th>
            <th>Passive Aggressive Classifier</th>
            <th>Perceptron</th>
            <th>Random Forest</th>
            <th>Extra Trees</th>
          </tr>
        </thead>
        <tbody>
          <tr class="odd">
            <td>kmeans</td>
            <td><em>0.952 ± 0.036</em></td>
            <td><em>0.92 ± 0.065</em></td>
            <td><em>0.944 ± 0.039</em></td>
            <td><em>0.945 ± 0.035</em></td>
            <td><strong>0.902 ± 0.043</strong></td>
            <td><em>0.871 ± 0.038</em></td>
            <td><em>0.853 ± 0.04</em></td>
          </tr>
          <tr class="even">
            <td>gmm</td>
            <td><em>0.936 ± 0.054</em></td>
            <td><em>0.946 ± 0.038</em></td>
            <td><em>0.943 ± 0.038</em></td>
            <td><em>0.929 ± 0.031</em></td>
            <td><em>0.882 ± 0.04</em></td>
            <td><strong>0.885 ± 0.022</strong></td>
            <td><strong>0.874 ± 0.026</strong></td>
          </tr>
          <tr class="odd">
            <td>bgmm</td>
            <td><em>0.955 ± 0.037</em></td>
            <td><em>0.932 ± 0.042</em></td>
            <td><em>0.945 ± 0.038</em></td>
            <td><em>0.939 ± 0.038</em></td>
            <td><em>0.896 ± 0.074</em></td>
            <td><em>0.86 ± 0.039</em></td>
            <td><em>0.87 ± 0.056</em></td>
          </tr>
          <tr class="even">
            <td>dbscan</td>
            <td>0.883 ± 0.027</td>
            <td>0.893 ± 0.031</td>
            <td>0.892 ± 0.033</td>
            <td>0.884 ± 0.027</td>
            <td>0.828 ± 0.064</td>
            <td>0.805 ± 0.064</td>
            <td>0.806 ± 0.058</td>
          </tr>
          <tr class="odd">
            <td>hierarchical</td>
            <td><strong>0.957 ± 0.032</strong></td>
            <td><strong>0.954 ± 0.038</strong></td>
            <td><strong>0.953 ± 0.038</strong></td>
            <td><strong>0.951 ± 0.032</strong></td>
            <td><em>0.891 ± 0.098</em></td>
            <td><em>0.881 ± 0.032</em></td>
            <td><em>0.872 ± 0.048</em></td>
          </tr>
        </tbody>
      </table>
      <figure>
        <img width="100%" src="results/fbirn_betas_accuracy.png?raw=True" />
        <img style="width:30%" src="results/accuracy_legend.png?raw=true" />
        <figcaption><b>Figure: </b> Fbirn Schizophrenia classification AUC scores using $\beta$-coefficients as
          features. The y-axis
          provides the AUC metric
          and the x-axis separates the scores obtained over 10-fold cross-validation for each classifier. </figcaption>
      </figure>
      <p>As expected, the reduced number of training features increased the accuracy of all the learners with Support
        Vector Machines acheiveing the highest accuracy across all clustering algorithms. Only DBCAN failed to get above
        an average AUC score of 0.90 for support vector machines and Hierarchical clusterting obtained this highest
        score
        of 0.957.</p>
      <hr />
      <h2 id="fbirn-data-visualizations">FNC State Analysis</h2>
      <h3> Kmeans Clustering </h3>
      <figure>
        <img width="98%" src="results/kmeans_fbirn_betas/kmeans_fbirn_states.png?raw=true" />
        <figcaption><b>Figure: </b> the FBIRN FNC states detected for each class using K-Means clustering. The top row
          are the
          states for healthy controls, and the bottom row are states for schizophrenic patients. These results follow
          closely with the results from Damaraju et al. 2014 [14], which indicates our pipeline works.</figcaption>
      </figure>
      <figure><img width="49%" src="results/kmeans_fbirn_betas/kmeans_fbirn_visualization_3d.png?raw=true" /> <img
          width="49%" src="results/kmeans_fbirn_betas/kmeans_fbirn_visualization.png?raw=true" /><br />
        <figcaption><b>Figure: </b>Visualization of FBIRN cluster-assignments to FNC windows for PCA-reduced windows in
          2-D
          and 3-D. Each color corresponds to one of the $k=5$ states.
        </figcaption>
      </figure>
      <hr />
      <h3> GMM Clustering </h3>
      <figure><img width="98%" src="results/gmm_fbirn_betas/gmm_fbirn_states.png?raw=true" /> Visualization of states
        with
        <figcaption><b>Figure: </b> the FBIRN FNC states detected for each class using GMM clustering. The top row are
          the states for healthy controls, and the bottom row are states for schizophrenic patients. We notice that
          states 2 and
          3 resemble states 3 and 4 from KMeans, and state 1 resembles state 2 from kmeans. State 0 and State 4
          most closely resemble state 0 from KMeans; however, both show greater negative connectivity in modules
          where it is not detected by KMeans.</figcaption>
      </figure>
      <figure><img width="49%" src="results/gmm_fbirn_betas/gmm_fbirn_visualization_3d.png?raw=true" /> <img width="49%"
          src="results/gmm_fbirn_betas/gmm_fbirn_visualization.png?raw=true" /><br />
        <figcaption><b>Figure: </b>Visualization of FBIRN cluster-assignments to FNC windows for PCA-reduced windows in
          2-D
          and 3-D. Each color corresponds to one of the $k=5$ states. We see that cluster 4 here is
          not detected by KMeans, perhaps indicating that the GMM has detected a enw state in the Fbirn
          data, which is still useful for schizophrenia diagnosis.
        </figcaption>
      </figure>
      <hr />
      <h3> Bayesian GMM </h3>
      <figure><img width="98%" src="results/bgmm_fbirn_betas/bgmm_fbirn_states.png?raw=true" /> Visualization of states
        with
        BGMM clustering with FBirn Data</figure>
      <figure><img width="49%" src="results/bgmm_fbirn_betas/bgmm_fbirn_visualization_3d.png?raw=true" /> <img
          width="49%" src="results/bgmm_fbirn_betas/bgmm_fbirn_visualization.png?raw=true" /><br />
        Visualization of clusters with BGMM clustering in 2-d and 3-d with FBirn Data</figure>
      <hr />
      <h3> DBSCAN Clustering </h3>
      <figure><img width="98%" src="results/dbscan_fbirn_betas/dbscan_fbirn_states.png?raw=true" /> Visualization of
        states
        with DBSCAN clustering with FBirn Data</figure>
      <figure><img width="49%" src="results/dbscan_fbirn_betas/dbscan_fbirn_visualization_3d.png?raw=true" /> <img
          width="49%" src="results/dbscan_fbirn_betas/dbscan_fbirn_visualization.png?raw=true" /><br />
        Visualization of clusters with DBSCAN clustering in 2-d and 3-d with FBirn Data</figure>
      <hr />
      <h3>Agglomerative Hierarchical Clustering</h3>
      <figure><img width="98%" src="results/hierarchical_fbirn_betas/hierarchical_fbirn_states.png?raw=true" />
        Visualization
        of states with Hierarchical clustering with FBirn Data</figure>
      <figure><img width="49%"
          src="results/hierarchical_fbirn_betas/hierarchical_fbirn_visualization_3d.png?raw=true" />
        <img width="49%" src="results/hierarchical_fbirn_betas/hierarchical_fbirn_visualization.png?raw=true" /><br />
        Visualization of clusters with Hierarchical clustering in 2-d and 3-d with FBirn Data</figure>

      <hr />
      <hr />


      <h1 id="section-v-conclusion">Section V: Conclusion</h1>
      <p>
        <ul>
          <li>
            <b>Cluster-Based features greatly improve classification</b> performance for the
            schizophrenia classification task.
          </li>
          <li>
            Classification of dFNC states <b>performs comporably when using GMM, Bayesian GMM, Hierarchical Clustering,
              and
              KMeans</b>
          </li>
          <li><b>GMM, bGMM, and Hierarchical clustering all detect states not detected with KMeans</b>, which provide
            either comparable or better classification of schizophrenia, indicating these states may have
            biological significance for diagnosis.
          </li>
          <li>
            <b>Hierarchical Clustering provides the highest classification results</b> for Fbirn, and GMM/bGMM provide
            the
            highest results for
            UCLA data, while also providing slightly different states than are detected with KMeans. These states
            may be
            useful for further
            neurological analysis.
          </li>
          <li>
            DBSCAN's sensitivity to parameters requires exhaustive hyper-parameter searching, without clear
            improvements
            in performance, but because states collected by DBSCAN in Fbirn data provided decent results (~80%
            accuracy),
            they may provide
            some biological insight.
          </li>
          <li>
            As established in literature, SVM classifiers perform most reliably for classifying dFNC states, which
            is
            consistent for both real data sets, and all clustering algorithms.
          </li>
        </ul>
      </p>
      <h2>Further Work</h2>
      <p>
        <ul>
          <li>Further neurological analysis of detected states is required for more thorough interpretation of
            results
          </li>
          <li>
            Elbow criterion for different clustering algorithms revealed some inconsistency in the use of $k=5$ for
            the
            number of states. Applying the full classification pipeline to other numbers of states may provide
            further
            insight
            into states, and improve classification even further.
          </li>
          <li>
            More advanced clustering and classification algorithms using Deep Learning may provide significant lifts
            classification. Particularly the use of Variational AutoEncoders for clustering seems a particularly
            promising
            avenue of future investigation.
          </li>
        </ul>
      </p>

      <h2 id="references">References</h2>
      <p><a name="ref1"></a> [1]. Eswar Damaraju et al. “Dynamic functional connectivity analysis reveals
        transient
        states
        of dyscon-nectivity in schizophrenia”. In:NeuroImage: Clinical5 (2014), pp. 298–308.<br></p>
      <p><a name="ref2"></a> [2]. Mustafa S Salman et al. “Group ICA for identifying biomarkers in
        schizophrenia:‘Adaptive’networks viaspatially constrained ICA show more sensitivity to group differences
        than
        spatio-temporal regression”.In:NeuroImage: Clinical22 (2019), p. 101747.<br></p>
      <p><a name="ref3"></a> [3]. Elena A Allen et al. “Tracking whole-brain connectivity dynamics in the resting
        state”.
        In:Cerebralcortex24.3 (2014), pp. 663–676.<br></p>
      <p><a name="ref4"></a> [4]. D. Zhi et al., “Abnormal Dynamic Functional Network Connectivity and Graph
        Theoretical
        Analysis in Major Depressive Disorder,” 2018 40th Annual International Conference of the IEEE Engineering
        in
        Medicine and Biology Society (EMBC), Honolulu, HI, 2018, pp. 558-561.<br></p>
      <p><a name="ref5"></a> [5]. U Sakoglu, AM Michael, and VD Calhoun. “Classification of schizophrenia patients
        vs
        healthy controlswith dynamic functional network connectivity”. In:Neuroimage47.1 (2009), S39–41.<br></p>
      <p><a name="ref6"></a> [6]. Unal Sako ̆glu et al. “A method for evaluating dynamic functional network
        connectivity
        and task-modulation: application to schizophrenia”. In:Magnetic Resonance Materials in Physics, Biology
        andMedicine23.5-6 (2010), pp. 351–366.<br></p>
      <p><a name="ref7"></a> [7]. V. M. Vergara, A. Abrol, F. A. Espinoza and V. D. Calhoun, &quot;Selection of
        Efficient
        Clustering Index to Estimate the Number of Dynamic Brain States from Functional Network
        Connectivity*,&quot;
        2019
        41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),
        Berlin,
        Germany, 2019, pp. 632-635.<br></p>
      <p><a name="ref8"></a> [8]. D. K. Saha, A. Abrol, E. Damaraju, B. Rashid, S. M. Plis and V. D. Calhoun,
        “Classification As a Criterion to Select Model Order For Dynamic Functional Connectivity States in
        Rest-fMRI
        Data,” 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019), Venice, Italy, 2019,
        pp. 1602-1605.<br></p>
      <p><a name="ref9"></a> [9]. Pedregosa et al. “2.3. Clustering.” Scikit,
        scikit-learn.org/stable/modules/clustering.html.<br></p>
      <p><a name="ref10"></a> [10]. Rashid, Barnaly, et al. “Classification of Schizophrenia and Bipolar Patients
        Using
        Static and Dynamic Resting-State FMRI Brain Connectivity.” NeuroImage, vol. 134, 2016, pp. 645–657.,
        doi:10.1016/j.neuroimage.2016.04.051.</p>
      <p><a name="ref11"></a> [11]. Bell, Anthony J., and Terrence J. Sejnowski. "An information-maximization
        approach
        to blind separation and blind
        deconvolution." Neural computation 7.6 (1995): 1129-1159.</p>
      <p><a name="ref12"></a>[12]. Du, Yuhui, and Yong Fan. "Group information guided ICA for fMRI data analysis."
        Neuroimage 69 (2013): 157-197.</p>
      <p><a name="ref13"></a> [13]. Salman, Mustafa S., et al. "Group information guided ICA shows more
        sensitivity to
        group differences than
        dual-regression." 2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). IEEE, 2017.
      </p>
      <p><a name="ref14"></a> [14]. Rashid, Barnaly, et al. "Classification of schizophrenia and bipolar patients
        using
        static and dynamic resting-state
        fMRI brain connectivity." Neuroimage 134 (2016): 645-657.</p>
      <p><a name="ref15"></a> [15]. Rachakonda, Srinivas, et al. "Group ICA of fMRI toolbox (GIFT) manual."
        Dostupnez
        [cit 2011-11-5] (2007).</p>
      <p><a name="ref16"></a>[16]. Du, Yuhui, et al. "NeuroMark: an adaptive independent component analysis
        framework
        for estimating reproducible and
        comparable fMRI biomarkers among brain disorders." medRxiv (2019): 19008631.</p>
      <p><a name="random_forests"> </a>[17]. Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.
      </p>
      <p><a name="extra_trees"></a>[18]. Simm J, de Abril I and Sugiyama M (2014). Tree-Based Ensemble Multi-Task
        Learning
        Method for Classification and
        Regression, volume 97 number 6</p>
      <p><a name="adaboost"></a>[19]. Y. Freund, and R. Schapire, “A Decision-Theoretic Generalization of On-Line
        Learning and
        an Application to Boosting”,
        1997.
      </p>
      <p><a name="bagging"></a>[20]. Breiman, Leo. "Bagging predictors." Machine learning 24.2 (1996): 123-140.
      </p>
      <p><a name="gradient_boost"></a>[21]. Friedman, Jerome H. "Stochastic gradient boosting." Computational
        statistics
        & data analysis 38.4 (2002): 367-378.</p>
      <p><a name="passive_aggressive"></a>[22] “Online Passive-Aggressive Algorithms” K. Crammer, O. Dekel, J.
        Keshat,
        S. Shalev-Shwartz, Y. Singer - JMLR 7 (2006)</p>
  </div>

  <h1>Appendices</h1>
  <h2>Appendix A: Gaussian Simulation Details</h2>
  <p>We derive our own synthetic dataset in order to test out our clusterers and classifiers on a simulated dataset
    for
    sanity checking their implementation.</p>
  <p>The data set was generated by a seed set of <span class="math inline"><em>M</em> = 50</span> source gaussian
    signals, which were organized into <span class="math inline"><em>k</em> = 5</span> “connectivity states”. Each of
    the 5 connectivity states is a block-diagonal matrix with a block of size <span
      class="math inline"><em>M</em>/<em>k</em></span> which is located on the <span
      class="math inline"><em>i</em> * <em>M</em>/<em>k</em> + <em>M</em>/<em>k</em></span>th entry with a size of
    <span class="math inline"><em>i</em> * <em>M</em>/<em>k</em></span>. For each state, the set the pairs of relevant
    source signals to come from the same distribution, so that their correlation without additional additive noise is
    1.
  </p>
  <p>For each subject, we generate a source signal of sime <span
      class="math inline"><em>M</em> × <em>T</em> − <em>W</em></span> where <span class="math inline"><em>T</em></span>
    is the length of the time-course, and <span class="math inline"><em>W</em></span> is the size of the sliding
    window.
    Over each window, we randomly select which state the window belongs to, drawing from a prior distribution of state
    probabilities for a subject’s class, <span class="math inline"><em>P</em><sub><em>c</em>, <em>k</em></sub></span>.
    Additionally, for each class we simulate a transition probability <span
      class="math inline"><em>Q</em><sub><em>c</em>, <em>k</em></sub></span> for each state, with higher probabilities
    meaning a higher chance of transitioning out of that state into some other state. Finally, we restrict the
    simulation such that transitions can only occur after a state has existed for a <span
      class="math inline"><em>W</em></span>-many timepoints. This restrictions allows us to assert with some certainty
    that each of the seed states will occur within some window for that subject.</p>
  <p>For each class, we simulate baseline signal noise from a normal distribution, which the relevant source signal
    being added to that baseline.</p>
  <p>Formally a single subjects signal within a window of size W is given as:</p>
  <p><br /><span
      class="math display"><em>X</em><sub><em>i</em></sub> = 𝒩(<em>θ</em><sub><em>c</em></sub>) + 𝒩(<em>θ</em><sub><em>k</em></sub>)</span><br />
  </p>
  <p>where <span class="math inline"><em>θ</em><sub><em>c</em></sub></span> and <span
      class="math inline"><em>θ</em><sub><em>k</em></sub></span> are the parameters for the baseline class signal for
    class <span class="math inline"><em>c</em></span> and the source signal for state <span
      class="math inline"><em>k</em></span> respectively. Because all source signals for state <span
      class="math inline"><em>k</em></span> will have the same distribution, their correlation will be high, providing
    the same block-matrix correlation for the dFNC analysis.</p>
  <p>The probability for entering from state <span class="math inline"><em>k</em></span> into a new state <span
      class="math inline"><em>k</em>′</span> at a timepoint <span class="math inline"><em>T</em></span>, given that
    the
    last transition occured <span class="math inline"><em>W</em></span> time-points ago, is given as the joint
    probability of the prior distribution for the class ever entering into state <span
      class="math inline"><em>k</em>′</span>, as well as the probability of transitioning out of state <span
      class="math inline"><em>k</em></span>. i.e. <br /><span
      class="math display"><em>Z</em><sub><em>c</em>, <em>k</em>, <em>k</em>′</sub>(<em>P</em><sub><em>c</em>, <em>k</em>′</sub>, <em>Q</em><sub><em>c</em>, <em>k</em></sub>)</span><br />
  </p>
  <p>The addition of noise, and the fact that windows are created with a size <span
      class="math inline"><em>W</em></span>
    at each timepoint means that there is a high chance for bleed-over between
    the actual states detected for an individual subject.</p>
  <p>For example the following connectivity matrices were computed from randomly selected subjects from each class,
    for
    each state:</p>
  <table>
    <thead>
      <tr class="header">
        <th style="text-align: center;">Class</th>
        <th style="text-align: center;">State 0</th>
        <th style="text-align: center;">State 1</th>
        <th style="text-align: center;">State 2</th>
        <th style="text-align: center;">State 3</th>
        <th style="text-align: center;">State 4</th>
      </tr>
    </thead>
    <tbody>
      <tr class="odd">
        <td style="text-align: center;">0</td>
        <td style="text-align: center;"><img width="100%" src="data/examples/gauss/class0_state0.png?raw=True" />
        </td>
        <td style="text-align: center;"><img width="100%" src="data/examples/gauss/class0_state1.png?raw=True" />
        </td>
        <td style="text-align: center;"><img width="100%" src="data/examples/gauss/class0_state2.png?raw=True" />
        </td>
        <td style="text-align: center;"><img width="100%" src="data/examples/gauss/class0_state3.png?raw=True" />
        </td>
        <td style="text-align: center;"><img width="100%" src="data/examples/gauss/class0_state4.png?raw=True" />
        </td>
      </tr>
      <tr class="even">
        <td style="text-align: center;">1</td>
        <td style="text-align: center;"><img width="100%" src="data/examples/gauss/class1_state0.png?raw=True" />
        </td>
        <td style="text-align: center;"><img width="100%" src="data/examples/gauss/class1_state1.png?raw=True" />
        </td>
        <td style="text-align: center;"><img width="100%" src="data/examples/gauss/class1_state2.png?raw=True" />
        </td>
        <td style="text-align: center;"><img width="100%" src="data/examples/gauss/class1_state3.png?raw=True" />
        </td>
        <td style="text-align: center;"><img width="100%" src="data/examples/gauss/class1_state4.png?raw=True" />
        </td>
      </tr>
    </tbody>
  </table>
  <p>
    <figcaption><b>Fig: </b> Examples of the 5 states for our simulated data set. Each of the states is a block
      matrix, with some mixture from adjacent states depending on the probability for that subject. E.g. in class one
      state 0 is highly probably, so it tends to appear mixed into other states, while in class 2, state 4 is highly
      probable,
      so it tends to mix in with other states.</figcaption>
  </p>

  <p>For our simulation we generate <span class="math inline"><em>C</em> = 2</span> classes, <span
      class="math inline"><em>K</em> = 5</span> states, for <span class="math inline"><em>N</em> = 300</span> subjects
    with <span class="math inline"><em>M</em> = 50</span> source signals. The parameters <span
      class="math inline"><em>Q</em></span> were set at 0.5 for each class and each state, so that transitioning out
    of
    states was equally likely for all classes and states. Baseline noise was set at <span
      class="math inline"><em>σ</em><sub><em>c</em></sub> = 1 × 10<sup> − 2</sup></span> for each class, and the <span
      class="math inline"><em>s</em><em>i</em><em>g</em><em>m</em><em>a</em><sub><em>k</em></sub></span> for all
    source
    signals was set at <span
      class="math inline"><em>s</em><em>i</em><em>g</em><em>m</em><em>a</em><sub><em>k</em></sub> = 1 × 10<sup> − 1</sup></span>.
  </p>
  <p>The parameters for <span class="math inline"><em>P</em></span> for each class were selected as:</p>
  <table>
    <thead>
      <tr class="header">
        <th>Class Parameter</th>
        <th>State 0</th>
        <th>State 1</th>
        <th>State 2</th>
        <th>State 3</th>
        <th>State 4</th>
      </tr>
    </thead>
    <tbody>
      <tr class="odd">
        <td><span class="math inline"><em>P</em><sub><em>c</em> = 0, <em>k</em></sub></span></td>
        <td>0.4</td>
        <td>0.1</td>
        <td>0.1</td>
        <td>0.3</td>
        <td>0.1</td>
      </tr>
      <tr class="even">
        <td>$P_{c=1,k}$</td>
        <td>0.1</td>
        <td>0.1</td>
        <td>0.3</td>
        <td>0.1</td>
        <td>0.4</td>
      </tr>
    </tbody>
  </table>

  <h2>Appendix B: Elbow Criterion</h2>
  <p>For K-Means, GMM, bGMM, and Agglomerative clustering, we measured the elbow criterion on a range of 2-9
    components. We measured both the correlation-distance dispersion, as is recommended in Damaraju et al. 2014 <a
      href="#ref1">[1]</a>,
    as
    well as the silhouette measure. The results from this analysis are included below. In general, we found either
    unclear or multiple elbows in the range <span class="math inline"><em>K</em> = 3, 4, 5, 6</span> for each data
    set, with the location of the elbow varying depending on the clustering algorithm used and the data set. Thus,
    we
    decided to compromise, and use the choice of <span class="math inline"><em>K</em> = 5</span> from Damaraju et
    al.,
    which is an established result for the Fbirn data set, and which is a common number of clusters chosen elswhere
    in
    the literature.</p>

  <table>
    <thead>
      <tr class="header">
        <th style="text-align: center;">Clustering Algorithm/
          Data Set</th>
        <th style="text-align: center;">Gaussian</th>
        <th style="text-align: center;">Fbirn</th>
        <th style="text-align: center;">UCLA</th>
      </tr>
    </thead>
    <tbody>
      <tr class="odd">
        <td style="text-align: center;">KMeans</td>
        <td style="text-align: center;"><img width="50%"
            src="results/kmeans_gauss_betas/kmeans_gauss_elbow.png?raw=True" /></td>
        <td style="text-align: center;"><img width="50%"
            src="results/kmeans_fbirn_betas/kmeans_fbirn_elbow.png?raw=True" /></td>
        <td style="text-align: center;"><img width="50%"
            src="results/kmeans_ucla_betas/kmeans_ucla_elbow.png?raw=True" />
        </td>
      </tr>
      <tr class="even">
        <td style="text-align: center;">GMM</td>
        <td style="text-align: center;"><img width="50%" src="results/gmm_gauss_betas/gmm_gauss_elbow.png?raw=True" />
        </td>
        <td style="text-align: center;"><img width="50%" src="results/gmm_fbirn_betas/gmm_fbirn_elbow.png?raw=True" />
        </td>
        <td style="text-align: center;"><img width="50%" src="results/gmm_ucla_betas/gmm_ucla_elbow.png?raw=True" />
        </td>
      </tr>
      <tr class="odd">
        <td style="text-align: center;">bGMM</td>
        <td style="text-align: center;"><img width="50%" src="results/bgmm_gauss_betas/bgmm_gauss_elbow.png?raw=True" />
        </td>
        <td style="text-align: center;"><img width="50%" src="results/bgmm_fbirn_betas/bgmm_fbirn_elbow.png?raw=True" />
        </td>
        <td style="text-align: center;"><img width="50%" src="results/bgmm_ucla_betas/bgmm_ucla_elbow.png?raw=True" />
        </td>
      </tr>
      <tr class="even">
        <td style="text-align: center;">Agglomerative</td>
        <td style="text-align: center;"><img width="50%"
            src="results/hierarchical_gauss_betas/hierarchical_gauss_elbow.png?raw=True" /></td>
        <td style="text-align: center;"><img width="50%"
            src="results/hierarchical_fbirn_betas/hierarchical_fbirn_elbow.png?raw=True" /></td>
        <td style="text-align: center;"><img width="50%"
            src="results/hierarchical_ucla_betas/hierarchical_ucla_elbow.png?raw=True" /></td>
      </tr>
    </tbody>
  </table>
  <p>
    <figcaption><b>Fig 3:</b> Table of elbow-criterion evaluations for all clustering algorithms using the
      correlation distance distortion, and the silhouette coefficients. In general, we see either no clear
      elbow, or multiple elbows, depending on the choice of clustering algorithm or data set.</figcaption>
  </p>

  <h2 id="ucla-dataset-1">Appendix C: UCLA Dataset Results</h2>
  <p>The UCAL Dataset was similarly trained. Firstly, it was trained using unclustered data to establish a
    baseline
    to
    compare against. The results are displayed below and as expected the supervised models failed to obtain
    suitable
    AUC scores.</p>
  <p><img src="images/ucla_pre_clustering_AUC.png?raw=true" width="75%" /></p>
  <p>The below plot shows “Accuracy” of various classifiers such as KMeans, Gaussian Mixture Model(GMM), Bayesian
    Gaussian Mixture Model(BGMM), Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and
    Hierarchical clustering methods on Simulated Gaussian Data using beta features. Accuracy has improved a lot in
    comparison to the previous case above without clusterer and without using beta feature say for example from
    0.7
    to
    0.9 for KMeans with “Multi Layer Perceptron” and likewise for other clusterer. Almost all the classifiers in
    all
    the clusterer shows improvement over KMeans clustering.</p>
  <table style="width:50%;">
    <colgroup>
      <col style="width: 12%" />
      <col style="width: 10%" />
      <col style="width: 13%" />
      <col style="width: 12%" />
      <col style="width: 18%" />
      <col style="width: 10%" />
      <col style="width: 10%" />
      <col style="width: 10%" />
    </colgroup>
    <thead>
      <tr class="header">
        <th>Clustering Algorithm</th>
        <th>SVM</th>
        <th>Multilayer Perceptron</th>
        <th>Logistic Regression</th>
        <th>Passive Aggressive Classifier</th>
        <th>Perceptron</th>
        <th>Extra Trees</th>
        <th>Random Forest</th>
      </tr>
    </thead>
    <tbody>
      <tr class="odd">
        <td>kmeans</td>
        <td><em>0.907 ± 0.057</em></td>
        <td><em>0.907 ± 0.057</em></td>
        <td><em>0.904 ± 0.06</em></td>
        <td><strong>0.896 ± 0.08</strong></td>
        <td><em>0.799 ± 0.116</em></td>
        <td><em>0.724 ± 0.168</em></td>
        <td><em>0.746 ± 0.133</em></td>
      </tr>
      <tr class="even">
        <td>gmm</td>
        <td><strong>0.91 ± 0.059</strong></td>
        <td><strong>0.909 ± 0.07</strong></td>
        <td><strong>0.908 ± 0.071</strong></td>
        <td><em>0.885 ± 0.087</em></td>
        <td><strong>0.886 ± 0.058</strong></td>
        <td><em>0.795 ± 0.095</em></td>
        <td><em>0.785 ± 0.108</em></td>
      </tr>
      <tr class="odd">
        <td>bgmm</td>
        <td><em>0.909 ± 0.075</em></td>
        <td><em>0.907 ± 0.081</em></td>
        <td><strong>0.908 ± 0.08</strong></td>
        <td><em>0.877 ± 0.105</em></td>
        <td><em>0.879 ± 0.081</em></td>
        <td><em>0.741 ± 0.157</em></td>
        <td><em>0.705 ± 0.166</em></td>
      </tr>
      <tr class="even">
        <td>dbscan</td>
        <td>0.409 ± 0.118</td>
        <td>0.467 ± 0.131</td>
        <td>0.69 ± 0.096</td>
        <td>0.667 ± 0.122</td>
        <td>0.5 ± 0.0</td>
        <td>0.643 ± 0.171</td>
        <td>0.649 ± 0.125</td>
      </tr>
      <tr class="odd">
        <td>hierarchical</td>
        <td><em>0.886 ± 0.054</em></td>
        <td><em>0.889 ± 0.07</em></td>
        <td><em>0.9 ± 0.069</em></td>
        <td><em>0.883 ± 0.071</em></td>
        <td><em>0.826 ± 0.122</em></td>
        <td><strong>0.829 ± 0.099</strong></td>
        <td><strong>0.792 ± 0.114</strong></td>
      </tr>
    </tbody>
  </table>
  <p><img width="86%" src="results/ucla_betas_accuracy.png?raw=True" /> <img width="12%"
      src="results/accuracy_legend.png?raw=true" /></p>

  <hr />
  <h2 id="ucla-data-visualisations">UCLA FNC State Analysis</h2>

  <h3> KMeans Clustering </h3>
  <figure><img width="98%" src="results/kmeans_ucla_betas/kmeans_ucla_states.png?raw=true" /> Visualization of
    states
    with
    KMeans clustering with UCLA Data</figure>
  <figure><img width="49%" src="results/kmeans_ucla_betas/kmeans_ucla_visualization_3d.png?raw=true" /> <img width="49%"
      src="results/kmeans_ucla_betas/kmeans_ucla_visualization.png?raw=true" /><br />
    Visualization of clusters with KMeans clustering in 2-d and 3-d with UCLA Data</figure>
  <hr />
  <h3> GMM Clustering </h3>
  <p><img width="98%" src="results/gmm_ucla_betas/gmm_ucla_states.png?raw=true" /> Visualization of states with
    GMM
    clustering with UCLA Data</p>
  <p><img width="49%" src="results/gmm_ucla_betas/gmm_ucla_visualization_3d.png?raw=true" /> <img width="49%"
      src="results/gmm_ucla_betas/gmm_ucla_visualization.png?raw=true" /><br />
    Visualization of clusters with GMM clustering in 2-d and 3-d with UCLA Data</p>
  <hr />
  <h3> Bayesian GMM Clustering </h3>
  <p><img width="98%" src="results/bgmm_ucla_betas/bgmm_ucla_states.png?raw=true" /> Visualization of states with
    BGMM
    clustering with UCLA Data</p>
  <p><img width="49%" src="results/bgmm_ucla_betas/bgmm_ucla_visualization_3d.png?raw=true" /> <img width="49%"
      src="results/bgmm_ucla_betas/bgmm_ucla_visualization.png?raw=true" /><br />
    Visualization of clusters with BGMM clustering in 2-d and 3-d with UCLA Data</p>
  <hr />
  <h3> DBSCAN Clustering </h3>
  <p><img width="49%" src="results/dbscan_ucla_betas/dbscan_ucla_visualization_3d.png?raw=true" /> <img width="49%"
      src="results/dbscan_ucla_betas/dbscan_ucla_visualization.png?raw=true" /><br />
    Visualization of clusters with DBSCAN clustering in 2-d and 3-d with UCLA Data</p>
  <hr />
  <h3> Agglomerative Hierarchical Clustering </h3>
  <p><img width="98%" src="results/hierarchical_ucla_betas/hierarchical_ucla_states.png?raw=true" /> Visualization
    of
    states with Hierarchical clustering with UCLA Data</p>
  <p><img width="49%" src="results/hierarchical_ucla_betas/hierarchical_ucla_visualization_3d.png?raw=true" />
    <img width="49%" src="results/hierarchical_ucla_betas/hierarchical_ucla_visualization.png?raw=true" /><br />
    Visualization of clusters with Hierarchical clustering in 2-d and 3-d with UCLA Data</p>
  <hr />
  <hr />

  <h2 id="gaussian-simulated-dataset-1">Appendix D: Gaussian Simulated Dataset Results</h2>

  <p>We initially trained the supervised learning algorithms using simulated Gaussian datasets since we did not
    have
    access to the real patient data until later in the semester. We first trained the supervised learning models
    without any clustering in order to establish a baseline AUC scores for our clustering algorithms to impove
    upon.
    These baseline results are displayed below.</p>
  <p><img src="images/sim_pre_clustering_AUC.png?raw=true" width="75%" /></p>
  <p>The key result from this experiment was that without any clustering, no supervised learning algorithm could
    consistently achieve AUC score above 0.60. Some learners such as the Multilayer Perceptron, Passive Aggressive
    Classifier, and Bernoulli Naive Bayes surprisingly scored below 0.40 suggesting that they consistently
    misdiagnose
    patients. This result was attributed to being due to random error, and was disregarded.</p>
  <p>After establishing the baseline AUC scores, we performed clustering and beta feature generation for the
    simulated
    Gaussian datasets. Using these clustered and reduced datasets, we trained across the same classifiers to for
    each
    clustering algorithm (K-Means, Gaussian Mixture Model, Bayesian Gaussian Mixture Model, DBSCAN, and
    Hierarchical)
    to generate the results below.</p>
  <table>
    <colgroup>
      <col style="width: 12%" />
      <col style="width: 12%" />
      <col style="width: 10%" />
      <col style="width: 9%" />
      <col style="width: 8%" />
      <col style="width: 8%" />
      <col style="width: 9%" />
      <col style="width: 11%" />
      <col style="width: 17%" />
    </colgroup>
    <thead>
      <tr class="header">
        <th>Clustering Algorithm</th>
        <th>Multilayer Perceptron</th>
        <th>Nearest Neighbors</th>
        <th>SVM</th>
        <th>Random Forest</th>
        <th>Extra Trees</th>
        <th>Gradient Boost</th>
        <th>Logistic Regression</th>
        <th>Passive Aggressive Classifier</th>
      </tr>
    </thead>
    <tbody>
      <tr class="odd">
        <td>kmeans</td>
        <td>0.972 ± 0.026</td>
        <td>0.96 ± 0.021</td>
        <td>0.972 ± 0.023</td>
        <td>0.962 ± 0.027</td>
        <td>0.966 ± 0.024</td>
        <td>0.954 ± 0.031</td>
        <td>0.971 ± 0.022</td>
        <td>0.974 ± 0.02</td>
      </tr>
      <tr class="even">
        <td>gmm</td>
        <td>0.963 ± 0.028</td>
        <td>0.954 ± 0.03</td>
        <td>0.972 ± 0.025</td>
        <td>0.967 ± 0.022</td>
        <td>0.976 ± 0.017</td>
        <td>0.928 ± 0.046</td>
        <td>0.962 ± 0.028</td>
        <td>0.97 ± 0.024</td>
      </tr>
      <tr class="odd">
        <td>bgmm</td>
        <td>0.966 ± 0.023</td>
        <td>0.949 ± 0.028</td>
        <td><em>0.974 ± 0.027</em></td>
        <td>0.966 ± 0.026</td>
        <td>0.963 ± 0.028</td>
        <td>0.962 ± 0.03</td>
        <td>0.974 ± 0.02</td>
        <td>0.972 ± 0.024</td>
      </tr>
      <tr class="even">
        <td>dbscan</td>
        <td>0.971 ± 0.024</td>
        <td>0.952 ± 0.025</td>
        <td>0.972 ± 0.022</td>
        <td>0.961 ± 0.024</td>
        <td>0.965 ± 0.023</td>
        <td>0.962 ± 0.022</td>
        <td>0.967 ± 0.025</td>
        <td>0.968 ± 0.026</td>
      </tr>
      <tr class="odd">
        <td>hierarchical</td>
        <td><strong>1.0 ± 0.0</strong></td>
        <td><strong>1.0 ± 0.0</strong></td>
        <td><strong>1.0 ± 0.0</strong></td>
        <td><strong>1.0 ± 0.0</strong></td>
        <td><strong>1.0 ± 0.0</strong></td>
        <td><strong>1.0 ± 0.001</strong></td>
        <td><strong>1.0 ± 0.0</strong></td>
        <td><strong>1.0 ± 0.0</strong></td>
      </tr>
    </tbody>
  </table>
  <table>
    <colgroup>
      <col style="width: 14%" />
      <col style="width: 10%" />
      <col style="width: 11%" />
      <col style="width: 12%" />
      <col style="width: 12%" />
      <col style="width: 15%" />
      <col style="width: 12%" />
      <col style="width: 12%" />
    </colgroup>
    <thead>
      <tr class="header">
        <th>Clustering Algorithm</th>
        <th>Perceptron</th>
        <th>Gaussian Process</th>
        <th>Ada Boost</th>
        <th>Voting</th>
        <th>Bernoulli Naive Bayes</th>
        <th>Bagging</th>
        <th>Decision Tree</th>
      </tr>
    </thead>
    <tbody>
      <tr class="odd">
        <td>kmeans</td>
        <td><em>0.947 ± 0.062</em></td>
        <td>0.955 ± 0.027</td>
        <td>0.957 ± 0.032</td>
        <td>0.92 ± 0.037</td>
        <td>0.948 ± 0.022</td>
        <td>0.941 ± 0.035</td>
        <td><em>0.938 ± 0.036</em></td>
      </tr>
      <tr class="even">
        <td>gmm</td>
        <td>0.962 ± 0.028</td>
        <td>0.952 ± 0.029</td>
        <td>0.955 ± 0.031</td>
        <td>0.92 ± 0.045</td>
        <td>0.938 ± 0.028</td>
        <td>0.917 ± 0.036</td>
        <td>0.923 ± 0.033</td>
      </tr>
      <tr class="odd">
        <td>bgmm</td>
        <td>0.969 ± 0.029</td>
        <td>0.955 ± 0.029</td>
        <td>0.958 ± 0.029</td>
        <td>0.923 ± 0.045</td>
        <td>0.949 ± 0.028</td>
        <td>0.931 ± 0.043</td>
        <td>0.933 ± 0.026</td>
      </tr>
      <tr class="even">
        <td>dbscan</td>
        <td>0.957 ± 0.035</td>
        <td>0.96 ± 0.032</td>
        <td>0.967 ± 0.028</td>
        <td>0.913 ± 0.034</td>
        <td>0.93 ± 0.031</td>
        <td>0.93 ± 0.036</td>
        <td>0.943 ± 0.022</td>
      </tr>
      <tr class="odd">
        <td>hierarchical</td>
        <td><strong>1.0 ± 0.0</strong></td>
        <td><strong>1.0 ± 0.001</strong></td>
        <td><strong>0.999 ± 0.002</strong></td>
        <td><strong>0.993 ± 0.014</strong></td>
        <td><strong>0.991 ± 0.009</strong></td>
        <td><strong>0.983 ± 0.016</strong></td>
        <td><strong>0.969 ± 0.035</strong></td>
      </tr>
    </tbody>
  </table>
  <p><img width="86%" src="results/gauss_betas_accuracy.png?raw=True" /> <img width="12%"
      src="results/accuracy_legend.png?raw=true" /></p>
  <p>The clustering combined with the beta feature generation dramatically improved the AUC scores on the
    simulated
    Gaussian datasets. All clustering algorithms produced AUC scores above 0.90 with standard deviations below
    0.05
    across all supervised models. The hierarchical clustering even produced perfect predictions. These results
    confirmed our suspicions that in order to accurately diagnose patients, we needed to perform clustering and
    reduce
    the number of features we trained our models on. These initial results using simulated data helped us
    tremendously
    when deploying our models on real patient data.</p>
  <h2 id="simulated-visualizations">Simulated State Analysis</h2>
  <h3>Kmeans Clustering</h3>
  <figure><img width="98%" src="results/kmeans_gauss_betas/kmeans_gauss_states.png?raw=true" /> Visualization of
    states
    with KMeans clustering with Gaussian Simulated Data</figure>
  <figure><img width="49%" src="results/kmeans_gauss_betas/kmeans_gauss_visualization_3d.png?raw=true" /> <img
      width="29%" src="results/kmeans_gauss_betas/kmeans_gauss_visualization.png?raw=true" /><br />
    Visualization of clusters with KMeans clustering in 2-d and 3-d with Gaussian Simulated Data</figure>
  <hr />
  <h3>GMM Clustering</h3>
  <figure><img width="98%" src="results/gmm_gauss_betas/gmm_gauss_states.png?raw=true" /> Visualization of states
    with
    GMM
    clustering with Gaussian Simulated Data</figure>
  <figure><img width="49%" src="results/gmm_gauss_betas/gmm_gauss_visualization_3d.png?raw=true" /> <img width="29%"
      src="results/gmm_gauss_betas/gmm_gauss_visualization.png?raw=true" /><br />
    Visualization of clusters with GMM clustering in 2-d and 3-d with Gaussian Simulated Data</figure>
  <hr />
  <h3>Bayesian GMM Clustering</h3>
  <figure><img width="98%" src="results/bgmm_gauss_betas/bgmm_gauss_states.png?raw=true" /> Visualization of
    states
    with
    BGMM clustering with Gaussian Simulated Data</figure>
  <figure><img width="49%" src="results/bgmm_gauss_betas/bgmm_gauss_visualization_3d.png?raw=true" /> <img width="29%"
      src="results/bgmm_gauss_betas/bgmm_gauss_visualization.png?raw=true" /><br />
    Visualization of clusters with BGMM clustering in 2-d and 3-d with Gaussian Simulated Data</figure>
  <hr />
  <h3>DBSCAN Clustering</h3>
  <figure><img width="98%" src="results/dbscan_gauss_betas/dbscan_gauss_states.png?raw=true" /> Visualization of
    states
    with DBSCAN clustering with Gaussian Simulated Data</figure>
  <figure><img width="49%" src="results/dbscan_gauss_betas/dbscan_gauss_visualization_3d.png?raw=true" /> <img
      width="29%" src="results/dbscan_gauss_betas/dbscan_gauss_visualization.png?raw=true" /><br />
    Visualization of clusters with DBSCAN clustering in 2-d and 3-d with Gaussian Simulated Data</figure>
  <hr />
  <h3>Agglomerative Hierarchical Clustering</h3>
  <figure><img width="98%" src="results/hierarchical_gauss_betas/hierarchical_gauss_states.png?raw=true" />
    Visualization
    of states with Hierarchical clustering with Gaussian Simulated Data</figure>
  <figure><img width="49%" src="results/hierarchical_gauss_betas/hierarchical_gauss_visualization_3d.png?raw=true" />
    <img width="49%" src="results/hierarchical_gauss_betas/hierarchical_gauss_visualization.png?raw=true" /><br />
    Visualization of clusters with Hierarchical clustering in 2-d and 3-d with Gaussian Simulated Data
  </figure>

  <h2 id="initial-ucla-significant-comparisons">Appendix E: Initial UCLA Significant Comparisons</h2>
  <p>In order to evaluate the predictive capacities of the features produced by each clustering algorithm, a
    two-tailed t-test was performed comparing the healthy control and the schizophrenic patients.</p>
  <p>The first t-test comparision was performed using the cluster assignments for each patient across the time
    domain
    where each time slot represented a feature of the training data. For each time slot (feature), the average
    cluster
    assignment for all the healthy control patients was compared to the average cluster assignment for all the
    schizophrenic patients. The corresponding p-values were then tested at a significance level of 0.10.</p>
  <p>The results displayed below highlight the points in time when there was less that a 10% chance that the
    observed
    difference in a healthy control’s cluster assignment and a schizophrenic’s cluster assignment was due to normal
    random variation. In theory, the more points of significance across time the more likely a trained model would
    accurately diagnose a subject. The results indicated that both K-Means and Gaussian Mixture Models failed to
    produce statistically different cluster assignments across time. The Bayesian Gaussian Mixture Model produced
    some
    significant differences while the Hierarchical clustering was significant at every time point.</p>
  <p>These results initially suggested that Hierarchical clustering would outperform all the other clustering
    algorithms, but the subsequent testing disporved this hypothesis.</p>
  <p>
    <figure>
      <img src="images/assignment_t_test_visualization.png?raw=true" />
      <figcaption><b>Figure: </b>Significance testing results for the four clustering algorithms on the UCLA data
        when using subject state-vectors as features. The x-axis is time, and the y-axis is whether or
        not a given timepoint was significant.
      </figcaption>
    </figure>
  </p>
  <p>Given the lack of improvement in accuracy across all clustering algorithms, it was believed that training
    supervised models using time points as features would require much more data for successful classification.
    Using
    time slots as features meant that there were 130 features in the data. Since there were only 267 patients in the
    UCLA data set, it was surmised that the dimensionality of the data was too high. To reduce the dimensionality of
    the datasets, the beta coefficients were calculated reducing the number of training features form 130 to 10.</p>

  <p>
    <figure>
      <img src="images/beta_t_test_visualization.png?raw=true" width="100%" />
      <figcaption><b>Figure: </b> Significance testing on the UCLA data set using $\beta$-coefficients for four of
        the
        clustering algorithms. We notice that all clustering algorithms find significant features, though
        which features are significant varies with the choice of algorithm. The x axis is the index of the feature
        in the
        $2*k$ $\beta$-space. The y axis is whether or not a given $\beta$-feature was significant. </figcaption>
    </figure>
  </p>

  <p>Each clustering algorithm produced statistically different beta coefficients between the health control and
    schizophrenic patients. With the reduced dimenstionality of the data, we successfully improved the accuracy of
    out
    diagnoses across all supervised learning algorithms for each clustering algorithm. These results are discussed
    later in the report.</p>

</body>

</html>
